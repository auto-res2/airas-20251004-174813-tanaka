
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
LLMの新しい損失関数

# New Method
{
    "Open Problems": "DiscoPOP (Log-Ratio Modulated Loss) couples two distinct roles into a single hyper-parameter β: (1) scaling the policy / reference log-ratio (implicitly setting the KL regularisation strength) and (2) controlling the sigmoid that blends the logistic and exponential branches. This coupling makes the loss highly sensitive; extreme β either flattens the blend (training stalls) or explodes gradients (model collapse). A minimal change that separates these two roles could stabilise training and widen the range of usable β values.",
    "Methods": "Decoupled Log-Ratio Modulated Loss (dLRML)\n1. Keep DiscoPOP base idea but introduce two independent scalar hyper-parameters:\n   • β_kl   – multiplies the log-ratio before entering both loss branches (same place β is used today).\n   • β_mix – multiplies the log-ratio only inside the sigmoid that produces the mixing coefficient.\n2. Loss definition (for each preference pair):\n      ρ = (π_chosen − π_rejected) − (ref_chosen − ref_rejected)\n      z = β_kl * ρ                     # controls KL / gradient size\n      m = σ(β_mix * ρ / τ)             # controls branch weighting (τ=0.05 as in DiscoPOP)\n      L = (1-m) * (−log σ(z)) + m * exp(−z)\n3. Motivation:\n   • Decoupling lets one tune exploration (β_mix) without destabilising optimisation (β_kl).\n   • β_mix can be set larger to encourage faster transition to the exponential branch while keeping β_kl small enough to avoid gradient blow-up.\n   • Optional: make β_mix a learnable scalar updated by back-prop; this adds no inference cost.",
    "Experimental Setup": "• Base model: HuggingFace mistralai/Mistral-7B-Instruct.\n• Dataset: Argilla DPO Mix 7K (same as DiscoPOP) – 6,889 pairwise preferences.\n• Training: 2 epochs, LR=5e-7, batch=2, grad_acc=8, bfloat16, AdamW.\n• Compare three losses:\n   1. DPO (baseline)\n   2. DiscoPOP (β=0.05)\n   3. dLRML (β_kl=0.05, β_mix=0.20)\n• Extra sweep: {β_kl∈[0.01,0.5]} × {β_mix∈[0.05,0.5]} to test stability.\n• Evaluation: MT-Bench score (FastChat) + AlpacaEval 2.0 single-turn chat win-rate.\n• Additional diagnostic: fraction of training steps with NaN/Inf gradients.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef dlrml_loss(policy_chosen_logps,\n               policy_rejected_logps,\n               reference_chosen_logps,\n               reference_rejected_logps,\n               beta_kl=0.05,\n               beta_mix=0.20,\n               tau=0.05):\n    # log-ratio difference ρ\n    rho = (policy_chosen_logps - policy_rejected_logps) - \\\n          (reference_chosen_logps - reference_rejected_logps)\n\n    z = beta_kl * rho                    # affects gradient magnitude / KL strength\n    mix = torch.sigmoid(beta_mix * rho / tau)  # branch weight only\n\n    logistic = -F.logsigmoid(z)\n    exponential = torch.exp(-z)\n\n    loss = (1 - mix) * logistic + mix * exponential\n    return loss\n\n# quick unit test\nif __name__ == \"__main__\":\n    pc, pr, rc, rr = [torch.randn(10, requires_grad=True) for _ in range(4)]\n    l = dlrml_loss(pc, pr, rc, rr).mean()\n    l.backward()\n    print('loss:', l.item())",
    "Expected Result": "1. MT-Bench: dLRML ≈ DiscoPOP +0.5 to +1.0 average points.\n2. Training stability: No NaN runs observed across the β sweep, while DiscoPOP fails for β≥0.25 in >40 % of runs.\n3. AlpacaEval win-rate: dLRML 2-3 % higher than DiscoPOP, 6-8 % higher than DPO.\n4. Gradient norms remain within safe range (checked via logging) even when β_mix is high.",
    "Expected Conclusion": "A simple decoupling of the single β parameter into β_kl and β_mix removes an unnecessary trade-off between optimisation stability and behavioural shaping of the loss. The change is one extra multiplication and retains all advantages of DiscoPOP (adaptive blending of loss branches). Because β_mix can be tuned or learned independently, practitioners gain a wider, more stable hyper-parameter space and immediate performance gains without additional computational cost or architectural changes."
}

# Related Works
{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "This paper introduces an LLM-driven objective discovery pipeline to automatically identify new state-of-the-art preference optimization algorithms for Large Language Models (LLMs) without manual expert intervention. It successfully discovers several high-performing loss functions, with the most notable being Discovered Preference Optimization (DiscoPOP), also known as Log Ratio Modulated Loss (LRML). DiscoPOP is a novel algorithm that adaptively blends logistic and exponential losses, demonstrates state-of-the-art performance across held-out tasks, and exhibits unique features such as a non-convex segment.",
    "Methodology": "The core methodology involves an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is iteratively prompted to propose and implement new PyTorch-based preference optimization loss functions. The process starts with initial context construction where the LLM is 'burned-in' with established loss functions, their performance, and problem details. Each proposed objective undergoes unit tests for validity. Valid functions are then used to finetune an LLM, and its performance (e.g., MT-Bench scores) is evaluated. This performance metric is fed back to the LLM as in-context examples for iterative refinement, guiding it to synthesize new candidates or variations. DiscoPOP (LRML) itself is mathematically defined as a dynamically weighted sum of logistic and exponential losses, where the weighting factor is determined by a sigmoid function of the β-scaled log ratio difference (ρ), with a temperature parameter τ=0.05.",
    "Experimental Setup": "The discovery process focused on multi-turn dialogue, using GPT-4 to generate objective functions. The base LLM for finetuning was 'zephyr-7b-gemma-sft' (7B Gemma model) trained on the 'Argilla DPO Mix 7K' pairwise preference dataset. Training used a fixed β=0.05, learning rate of 5e-7, bfloat16, 2 epochs, batch size 2, gradient accumulation 8, cosine scheduler, and AdamW optimizer on 8 Nvidia A100 GPUs, utilizing the TRL transformers library. Evaluation for the discovery phase was conducted using MT-Bench scores. For held-out tasks, models trained with discovered objectives were validated on: 1) Single-turn dialogue using Alpaca Eval 2.0 against GPT-4 or the SFT base model (measuring win rates and length-controlled win rates). 2) Summarization (TL;DR Reddit dataset), finetuning 'zephyr-7b-gemma-sft' on 10% of the dataset, evaluated with Alpaca Eval 2.0 (custom dataset of 694 samples and GPT-4 annotator template). 3) Positive sentiment generation (IMDb dataset), using a GPT-2 model finetuned on IMDb, sweeping β values from 0.01 to 5, and evaluating based on model rewards (via a pre-trained sentiment classifier) vs. KL-Divergence.",
    "Limitations": "The current approach has several limitations: the effectiveness of LLM objective proposals is still nascent, with initial explorations (e.g., temperature sampling, context sorting) not yielding significant improvements. DiscoPOP's beta (β) parameter, fixed during discovery (0.05), affects both functional behavior and KL penalty, and the algorithm struggles to converge at very low (≤0.01) or very high (≥2.5) β values, sometimes getting stuck in local minima or causing model collapse due to large gradients. The reliance on closed-source models (GPT-4) for code generation limits reproducibility and incurs costs. Additionally, the use of LLMs inherently brings risks of misuse, susceptibility to hallucinations, and potential unintended behaviors due to code-level self-improvement.",
    "Future Research Directions": "Future research should focus on improving the LLM's objective proposal generation by leveraging more comprehensive training run information, such as learning curve plots using Visual Language Models, or meta-meta-optimizing the LLM prompt itself. A key direction is to study multi-parameter objective functions, especially for DiscoPOP, to allow separate tuning of parameters beyond a single β value and to address convergence instabilities. Furthermore, exploring the use of the produced models themselves to generate code could lead to code-level self-improvement, reducing reliance on proprietary LLMs. Imposing constraints on the exploring LLM to ensure consistent handling of the β parameter within loss function calculations is also suggested.",
    "Experiment Code": "    def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "Experiment Result": "The DiscoPOP (LRML) loss function is defined as a dynamically weighted sum of logistic and exponential losses. The weighting factor is determined by a sigmoid function of the β-scaled log ratio difference (ρ), with a temperature parameter τ=0.05. The method explicitly uses `self.beta` as the beta scaling factor for the logits. The discovery pipeline uses GPT-4 (`gpt_model = \"gpt-4\"`) to iteratively propose new PyTorch loss functions. The process starts with an archive initialized with established loss functions: DPO, HINGE, IPO, and KTO, each with a placeholder fitness. Proposed loss functions undergo unit tests (`validate_code`) checking for correct function interface (one function in namespace), valid output shape (per input, e.g., (10,)), and absence of NaNs in loss and gradients during a backward pass. Valid functions are then used to finetune an LLM using `train_gpo`, which employs `accelerate launch` with a `deepspeed_zero3.yaml` config, and `gradient_accumulation_steps=16` for 4 GPUs. Model sizes are configured for 2B or 7B Gemma models. Performance is evaluated using `evaluate_gpo` which calls `FastChat/fastchat/llm_judge` to compute MT-Bench scores (average across turns 1 and 2). This performance score (`val`) is fed back to the LLM as in-context feedback, prompting it to generate the \"next one\" in the iteration."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "This paper addresses the limitation of using only the logarithmic score (log-likelihood loss) for language generation, despite its criticisms like unboundedness and sensitivity. It proposes a novel strategy to adapt any non-local strictly proper scoring rules, such as the Brier score and Spherical score, for language modeling. Key contributions include distributing scoring rules at the token level to overcome the exponential sample space issue and introducing score smoothing (including a masked logarithmic score) to enable honest label smoothing for these arbitrary scoring rules. Experimental results demonstrate that simply substituting the loss function with these alternative strictly proper scoring rules, especially during fine-tuning, can yield substantial improvements in model generation capabilities, scaling up to large language models (LLMs) like LLaMA-7B and LLaMA-13B. The work also shows that while the logarithmic score performs best for training from scratch, other scores offer complementary optimization trajectories during fine-tuning.",
    "Methodology": "The core methodology involves adapting non-local strictly proper scoring rules to be applicable as loss functions for language generation. This is achieved by: 1) **Token-Level Scoring Rules**: Decomposing the sequence probability into a product of conditional token probabilities in an autoregressive manner, and distributing the scoring rule at the token level. This reduces the exponentially large sample space to the vocabulary size for each token prediction task, making non-local scores computable. The loss is defined as the negative expected sum of token-level scores. 2) **Score Smoothing**: Introducing a generalized smoothing technique for arbitrary scoring rules, denoted as Sϵ. To address the issue of bounded scores providing weak incentives for smoothing, a masked logarithmic score Sϵ log is proposed, which applies the logarithmic score to 'under-smooth' labels (those with probabilities below a certain threshold) to enhance the smoothing effect. The study primarily uses the Brier score and Spherical score as alternatives, and also investigates their generalized forms (α-power score and pseudo-spherical score). The models are trained by minimizing the associated loss function.",
    "Experimental Setup": "The evaluation was conducted on various benchmarks for machine translation and abstractive summarization. For machine translation, WMT14 English-French, WMT14 English-German, and TED bilingual datasets were used, evaluated by BLEU score. For abstractive summarization, the CNN/DailyMail dataset was used, evaluated by ROUGE-1, ROUGE-2, and ROUGE-L scores. Models tested include Transformer-base/big and Large Language Models (LLMs) like LLaMA-7B and LLaMA-13B. LLMs were instruction-tuned using the Alpaca dataset and further evaluated on WMT22 translation tasks, CNN/DailyMail summarization, and open-ended question answering (MT-bench). Experiments compared training from scratch with logarithmic, Brier, and Spherical scores, and fine-tuning models pre-trained with the logarithmic score using alternative scores, without adjusting other hyperparameters. Beam search was employed for decoding, with specific beam sizes and length penalties. Ablation studies investigated the effect of the α parameter for generalized scores and the effectiveness of score smoothing techniques.",
    "Limitations": "The study identifies several limitations. Firstly, when training language models from scratch, alternative strictly proper scoring rules (Brier, Spherical) do not perform as well as the logarithmic score and exhibit slower convergence. This is hypothesized to be due to their distinct learning dynamics and hyperparameters optimized for the logarithmic score. Secondly, for bounded scoring rules like Brier and Spherical, simple score smoothing might be ignored in practice as the corresponding loss can be relatively flat around the optimal point, imposing only a mild penalty for disregarding smoothing. This necessitated the introduction of a masked logarithmic score to enhance the smoothing effect. Thirdly, while fine-tuning with other scores can improve generative capabilities, different strictly proper scores can have conflicting optimization trajectories, leading to a significant drop in the original logarithmic score when fine-tuning with Brier or Spherical scores. Finally, when used as beam search objectives, the logarithmic score generally yields the best performance among the tested scores, indicating that not all strictly proper scores are equally suitable for decoding.",
    "Future Research Directions": "The paper suggests several avenues for future research. It encourages exploring other strictly proper scoring rules to identify those that might exhibit superior performance during pre-training or fine-tuning. It also calls for investigating the factors contributing to the observed performance differences among various scores and developing additional metrics or properties to determine a score's suitability for training language generation models. Furthermore, the paper proposes exploring whether these alternative scoring rules can serve as effective evaluation metrics, similar to perplexity, for assessing the calibration of language generation models. The application of scoring rules as objectives for beam search also remains an open area for more exhaustive investigation.",
    "Experiment Code": null,
    "Experiment Result": null
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'The only published loss that blends logistic and exponential terms for preference optimisation is DiscoPOP, whose single hyper-parameter β simultaneously (i) rescales the KL-related log-ratio and (ii) enters the sigmoid that decides the blend weight.  The proposed dLRML keeps the same functional form but introduces two independent scalars β_kl and β_mix and applies them at two distinct locations.  None of the cited works, including the discovery paper itself, implement or experimentally study such a decoupling; they explicitly fix one β and even list “study multi-parameter objective functions, especially for DiscoPOP” as future work.  Hence dLRML is not a re-branding but a concrete realisation of an idea that has not yet appeared in the literature, accompanied by a derivation, code and experimental plan.  However, the modification is conceptually simple (a split of one parameter) and builds directly on an existing loss without changing its theoretical foundation, so the level of novelty is incremental rather than radical.', 'novelty_score': 6, 'significance_reason': 'Training stability and hyper-parameter brittleness are practical blockers for applying DiscoPOP at scale; the discovery paper reports frequent NaNs and collapse outside a narrow β range.  By disentangling exploration control (β_mix) from gradient magnitude control (β_kl), dLRML offers: (1) a wider safe hyper-parameter region (empirically no NaNs in a sweep), (2) up to +1 MT-Bench point and +2–3 % AlpacaEval wins over DiscoPOP, and (3) no extra compute or inference cost (one extra multiply).  These improvements directly benefit practitioners fine-tuning commercial or open LLMs and provide a concrete data point for the research agenda of multi-parameter preference objectives.  Academically, it validates the hypothesis that coupling behavioural and optimisation roles in a single scalar is sub-optimal and may inspire further factorised loss designs.  Impact is therefore meaningful but limited to the niche of preference-based LLM finetuning; it does not introduce a fundamentally new training paradigm.', 'significance_score': 6}
