
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    LLMの新しい損失関数
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "This paper introduces an LLM-driven objective discovery pipeline to automatically identify new state-of-the-art preference optimization algorithms for Large Language Models (LLMs) without manual expert intervention. It successfully discovers several high-performing loss functions, with the most notable being Discovered Preference Optimization (DiscoPOP), also known as Log Ratio Modulated Loss (LRML). DiscoPOP is a novel algorithm that adaptively blends logistic and exponential losses, demonstrates state-of-the-art performance across held-out tasks, and exhibits unique features such as a non-convex segment.",
    "Methodology": "The core methodology involves an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is iteratively prompted to propose and implement new PyTorch-based preference optimization loss functions. The process starts with initial context construction where the LLM is 'burned-in' with established loss functions, their performance, and problem details. Each proposed objective undergoes unit tests for validity. Valid functions are then used to finetune an LLM, and its performance (e.g., MT-Bench scores) is evaluated. This performance metric is fed back to the LLM as in-context examples for iterative refinement, guiding it to synthesize new candidates or variations. DiscoPOP (LRML) itself is mathematically defined as a dynamically weighted sum of logistic and exponential losses, where the weighting factor is determined by a sigmoid function of the β-scaled log ratio difference (ρ), with a temperature parameter τ=0.05.",
    "Experimental Setup": "The discovery process focused on multi-turn dialogue, using GPT-4 to generate objective functions. The base LLM for finetuning was 'zephyr-7b-gemma-sft' (7B Gemma model) trained on the 'Argilla DPO Mix 7K' pairwise preference dataset. Training used a fixed β=0.05, learning rate of 5e-7, bfloat16, 2 epochs, batch size 2, gradient accumulation 8, cosine scheduler, and AdamW optimizer on 8 Nvidia A100 GPUs, utilizing the TRL transformers library. Evaluation for the discovery phase was conducted using MT-Bench scores. For held-out tasks, models trained with discovered objectives were validated on: 1) Single-turn dialogue using Alpaca Eval 2.0 against GPT-4 or the SFT base model (measuring win rates and length-controlled win rates). 2) Summarization (TL;DR Reddit dataset), finetuning 'zephyr-7b-gemma-sft' on 10% of the dataset, evaluated with Alpaca Eval 2.0 (custom dataset of 694 samples and GPT-4 annotator template). 3) Positive sentiment generation (IMDb dataset), using a GPT-2 model finetuned on IMDb, sweeping β values from 0.01 to 5, and evaluating based on model rewards (via a pre-trained sentiment classifier) vs. KL-Divergence.",
    "Limitations": "The current approach has several limitations: the effectiveness of LLM objective proposals is still nascent, with initial explorations (e.g., temperature sampling, context sorting) not yielding significant improvements. DiscoPOP's beta (β) parameter, fixed during discovery (0.05), affects both functional behavior and KL penalty, and the algorithm struggles to converge at very low (≤0.01) or very high (≥2.5) β values, sometimes getting stuck in local minima or causing model collapse due to large gradients. The reliance on closed-source models (GPT-4) for code generation limits reproducibility and incurs costs. Additionally, the use of LLMs inherently brings risks of misuse, susceptibility to hallucinations, and potential unintended behaviors due to code-level self-improvement.",
    "Future Research Directions": "Future research should focus on improving the LLM's objective proposal generation by leveraging more comprehensive training run information, such as learning curve plots using Visual Language Models, or meta-meta-optimizing the LLM prompt itself. A key direction is to study multi-parameter objective functions, especially for DiscoPOP, to allow separate tuning of parameters beyond a single β value and to address convergence instabilities. Furthermore, exploring the use of the produced models themselves to generate code could lead to code-level self-improvement, reducing reliance on proprietary LLMs. Imposing constraints on the exploring LLM to ensure consistent handling of the β parameter within loss function calculations is also suggested.",
    "Experiment Code": "    def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "Experiment Result": "The DiscoPOP (LRML) loss function is defined as a dynamically weighted sum of logistic and exponential losses. The weighting factor is determined by a sigmoid function of the β-scaled log ratio difference (ρ), with a temperature parameter τ=0.05. The method explicitly uses `self.beta` as the beta scaling factor for the logits. The discovery pipeline uses GPT-4 (`gpt_model = \"gpt-4\"`) to iteratively propose new PyTorch loss functions. The process starts with an archive initialized with established loss functions: DPO, HINGE, IPO, and KTO, each with a placeholder fitness. Proposed loss functions undergo unit tests (`validate_code`) checking for correct function interface (one function in namespace), valid output shape (per input, e.g., (10,)), and absence of NaNs in loss and gradients during a backward pass. Valid functions are then used to finetune an LLM using `train_gpo`, which employs `accelerate launch` with a `deepspeed_zero3.yaml` config, and `gradient_accumulation_steps=16` for 4 GPUs. Model sizes are configured for 2B or 7B Gemma models. Performance is evaluated using `evaluate_gpo` which calls `FastChat/fastchat/llm_judge` to compute MT-Bench scores (average across turns 1 and 2). This performance score (`val`) is fed back to the LLM as in-context feedback, prompting it to generate the \"next one\" in the iteration."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "This paper addresses the limitation of using only the logarithmic score (log-likelihood loss) for language generation, despite its criticisms like unboundedness and sensitivity. It proposes a novel strategy to adapt any non-local strictly proper scoring rules, such as the Brier score and Spherical score, for language modeling. Key contributions include distributing scoring rules at the token level to overcome the exponential sample space issue and introducing score smoothing (including a masked logarithmic score) to enable honest label smoothing for these arbitrary scoring rules. Experimental results demonstrate that simply substituting the loss function with these alternative strictly proper scoring rules, especially during fine-tuning, can yield substantial improvements in model generation capabilities, scaling up to large language models (LLMs) like LLaMA-7B and LLaMA-13B. The work also shows that while the logarithmic score performs best for training from scratch, other scores offer complementary optimization trajectories during fine-tuning.",
    "Methodology": "The core methodology involves adapting non-local strictly proper scoring rules to be applicable as loss functions for language generation. This is achieved by: 1) **Token-Level Scoring Rules**: Decomposing the sequence probability into a product of conditional token probabilities in an autoregressive manner, and distributing the scoring rule at the token level. This reduces the exponentially large sample space to the vocabulary size for each token prediction task, making non-local scores computable. The loss is defined as the negative expected sum of token-level scores. 2) **Score Smoothing**: Introducing a generalized smoothing technique for arbitrary scoring rules, denoted as Sϵ. To address the issue of bounded scores providing weak incentives for smoothing, a masked logarithmic score Sϵ log is proposed, which applies the logarithmic score to 'under-smooth' labels (those with probabilities below a certain threshold) to enhance the smoothing effect. The study primarily uses the Brier score and Spherical score as alternatives, and also investigates their generalized forms (α-power score and pseudo-spherical score). The models are trained by minimizing the associated loss function.",
    "Experimental Setup": "The evaluation was conducted on various benchmarks for machine translation and abstractive summarization. For machine translation, WMT14 English-French, WMT14 English-German, and TED bilingual datasets were used, evaluated by BLEU score. For abstractive summarization, the CNN/DailyMail dataset was used, evaluated by ROUGE-1, ROUGE-2, and ROUGE-L scores. Models tested include Transformer-base/big and Large Language Models (LLMs) like LLaMA-7B and LLaMA-13B. LLMs were instruction-tuned using the Alpaca dataset and further evaluated on WMT22 translation tasks, CNN/DailyMail summarization, and open-ended question answering (MT-bench). Experiments compared training from scratch with logarithmic, Brier, and Spherical scores, and fine-tuning models pre-trained with the logarithmic score using alternative scores, without adjusting other hyperparameters. Beam search was employed for decoding, with specific beam sizes and length penalties. Ablation studies investigated the effect of the α parameter for generalized scores and the effectiveness of score smoothing techniques.",
    "Limitations": "The study identifies several limitations. Firstly, when training language models from scratch, alternative strictly proper scoring rules (Brier, Spherical) do not perform as well as the logarithmic score and exhibit slower convergence. This is hypothesized to be due to their distinct learning dynamics and hyperparameters optimized for the logarithmic score. Secondly, for bounded scoring rules like Brier and Spherical, simple score smoothing might be ignored in practice as the corresponding loss can be relatively flat around the optimal point, imposing only a mild penalty for disregarding smoothing. This necessitated the introduction of a masked logarithmic score to enhance the smoothing effect. Thirdly, while fine-tuning with other scores can improve generative capabilities, different strictly proper scores can have conflicting optimization trajectories, leading to a significant drop in the original logarithmic score when fine-tuning with Brier or Spherical scores. Finally, when used as beam search objectives, the logarithmic score generally yields the best performance among the tested scores, indicating that not all strictly proper scores are equally suitable for decoding.",
    "Future Research Directions": "The paper suggests several avenues for future research. It encourages exploring other strictly proper scoring rules to identify those that might exhibit superior performance during pre-training or fine-tuning. It also calls for investigating the factors contributing to the observed performance differences among various scores and developing additional metrics or properties to determine a score's suitability for training language generation models. Furthermore, the paper proposes exploring whether these alternative scoring rules can serve as effective evaluation metrics, similar to perplexity, for assessing the calibration of language generation models. The application of scoring rules as objectives for beam search also remains an open area for more exhaustive investigation.",
    "Experiment Code": null,
    "Experiment Result": null
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "DiscoPOP (Log-Ratio Modulated Loss) couples two distinct roles into a single hyper-parameter β: (1) scaling the policy / reference log-ratio (implicitly setting the KL regularisation strength) and (2) controlling the sigmoid that blends the logistic and exponential branches. This coupling makes the loss highly sensitive; extreme β either flattens the blend (training stalls) or explodes gradients (model collapse). A minimal change that separates these two roles could stabilise training and widen the range of usable β values.",
    "methods": "Decoupled Log-Ratio Modulated Loss (dLRML)\n1. Keep DiscoPOP base idea but introduce two independent scalar hyper-parameters:\n   • β_kl   – multiplies the log-ratio before entering both loss branches (same place β is used today).\n   • β_mix – multiplies the log-ratio only inside the sigmoid that produces the mixing coefficient.\n2. Loss definition (for each preference pair):\n      ρ = (π_chosen − π_rejected) − (ref_chosen − ref_rejected)\n      z = β_kl * ρ                     # controls KL / gradient size\n      m = σ(β_mix * ρ / τ)             # controls branch weighting (τ=0.05 as in DiscoPOP)\n      L = (1-m) * (−log σ(z)) + m * exp(−z)\n3. Motivation:\n   • Decoupling lets one tune exploration (β_mix) without destabilising optimisation (β_kl).\n   • β_mix can be set larger to encourage faster transition to the exponential branch while keeping β_kl small enough to avoid gradient blow-up.\n   • Optional: make β_mix a learnable scalar updated by back-prop; this adds no inference cost.",
    "experimental_setup": "• Base model: HuggingFace mistralai/Mistral-7B-Instruct.\n• Dataset: Argilla DPO Mix 7K (same as DiscoPOP) – 6,889 pairwise preferences.\n• Training: 2 epochs, LR=5e-7, batch=2, grad_acc=8, bfloat16, AdamW.\n• Compare three losses:\n   1. DPO (baseline)\n   2. DiscoPOP (β=0.05)\n   3. dLRML (β_kl=0.05, β_mix=0.20)\n• Extra sweep: {β_kl∈[0.01,0.5]} × {β_mix∈[0.05,0.5]} to test stability.\n• Evaluation: MT-Bench score (FastChat) + AlpacaEval 2.0 single-turn chat win-rate.\n• Additional diagnostic: fraction of training steps with NaN/Inf gradients.",
    "experimental_code": "import torch, torch.nn.functional as F\n\ndef dlrml_loss(policy_chosen_logps,\n               policy_rejected_logps,\n               reference_chosen_logps,\n               reference_rejected_logps,\n               beta_kl=0.05,\n               beta_mix=0.20,\n               tau=0.05):\n    # log-ratio difference ρ\n    rho = (policy_chosen_logps - policy_rejected_logps) - \\\n          (reference_chosen_logps - reference_rejected_logps)\n\n    z = beta_kl * rho                    # affects gradient magnitude / KL strength\n    mix = torch.sigmoid(beta_mix * rho / tau)  # branch weight only\n\n    logistic = -F.logsigmoid(z)\n    exponential = torch.exp(-z)\n\n    loss = (1 - mix) * logistic + mix * exponential\n    return loss\n\n# quick unit test\nif __name__ == \"__main__\":\n    pc, pr, rc, rr = [torch.randn(10, requires_grad=True) for _ in range(4)]\n    l = dlrml_loss(pc, pr, rc, rr).mean()\n    l.backward()\n    print('loss:', l.item())",
    "expected_result": "1. MT-Bench: dLRML ≈ DiscoPOP +0.5 to +1.0 average points.\n2. Training stability: No NaN runs observed across the β sweep, while DiscoPOP fails for β≥0.25 in >40 % of runs.\n3. AlpacaEval win-rate: dLRML 2-3 % higher than DiscoPOP, 6-8 % higher than DPO.\n4. Gradient norms remain within safe range (checked via logging) even when β_mix is high.",
    "expected_conclusion": "A simple decoupling of the single β parameter into β_kl and β_mix removes an unnecessary trade-off between optimisation stability and behavioural shaping of the loss. The change is one extra multiplication and retains all advantages of DiscoPOP (adaptive blending of loss branches). Because β_mix can be tuned or learned independently, practitioners gain a wider, more stable hyper-parameter space and immediate performance gains without additional computational cost or architectural changes."
}
