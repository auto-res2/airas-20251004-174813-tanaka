
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The core methodology involves an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is iteratively prompted to propose and implement new PyTorch-based preference optimization loss functions. The process starts with initial context construction where the LLM is 'burned-in' with established loss functions, their performance, and problem details. Each proposed objective undergoes unit tests for validity. Valid functions are then used to finetune an LLM, and its performance (e.g., MT-Bench scores) is evaluated. This performance metric is fed back to the LLM as in-context examples for iterative refinement, guiding it to synthesize new candidates or variations. DiscoPOP (LRML) itself is mathematically defined as a dynamically weighted sum of logistic and exponential losses, where the weighting factor is determined by a sigmoid function of the β-scaled log ratio difference (ρ), with a temperature parameter τ=0.05.

# Repository Content
File Path: scripts/create_tldr_dataset.py
Content:
import numpy as np
import pandas as pd
import torch
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

from alignment import (
    DataArguments,
    DPOConfig,
    H4ArgumentParser,
    ModelArguments,
    get_kbit_device_map,
    get_quantization_config,
)


# create the top-level parser
parser = H4ArgumentParser((ModelArguments, DataArguments, DPOConfig))
model_args, data_args, training_args = parser.parse()

model_args, data_args, training_args = parser.parse()

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    # if args.n_gpu > 0:
    torch.cuda.manual_seed_all(seed)


set_seed(42)


torch_dtype = (
    model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
)
quantization_config = get_quantization_config(model_args)

model_kwargs = dict(
    revision=model_args.model_revision,
    trust_remote_code=model_args.trust_remote_code,
    use_flash_attention_2=model_args.use_flash_attention_2,
    torch_dtype=torch_dtype,
    use_cache=False if training_args.gradient_checkpointing else True,
    device_map=get_kbit_device_map() if quantization_config is not None else None,
    quantization_config=quantization_config,
)

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6b")
# tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)
model.to("cuda")
# Load reference model
ref_model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6b", **model_kwargs)
# ref_model.load_state_dict(torch.load(ref_model_name, map_location=torch.device('cpu'))['state'])
ref_model.to("cuda")
# import ipdb; ipdb.set_trace()

# Load the TLDR dataset
tldr_test = load_dataset("CarperAI/openai_summarize_comparisons", split="test[:5%]")

df = pd.DataFrame()

instructions = []
outputs = []
for i, datapoint in tqdm(enumerate(tldr_test)):
    if i % 6 == 0:
        prompt = f"{datapoint['prompt']}\n\nTL;DR:"
        chosen = str(datapoint["chosen"])
        instructions.append(prompt)
        outputs.append(chosen)

df["instruction"] = instructions
df["output"] = outputs
df["dataset"] = "tldr"
df["generator"] = "human"

print(len(tldr_test))

df.to_csv("./alpaca_eval/tldr_eval/tldr_dataset.csv")


# # Preprocess the dataset
# eval_prompts = tldr_test["prompt"]
# inputs = tokenizer(eval_prompts, return_tensors='pt', truncation=True, padding=True)

# # Prepare for batching
# dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], )
# print(len(dataset))
# data_loader = DataLoader(dataset, batch_size=4)


# with torch.no_grad():
#     for batch_input_ids, batch_attention_mask in tqdm(data_loader):
#         print("input: ", tokenizer.batch_decode(batch_input_ids, skip_special_tokens=True))
#         # Generate samples from the pretrained model
#         batch_input_ids = batch_input_ids.cuda()
#         batch_attention_mask = batch_attention_mask.cuda()
#         # with torch.no_grad():
#         generated_ids = model.generate(batch_input_ids,
#                                        attention_mask=batch_attention_mask,
#                                        do_sample=True,
#                                        max_new_tokens=256,
#                                        pad_token_id=tokenizer.pad_token_id)

#         generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
#         print("output: ", generated_texts)
#         exit()

File Path: scripts/launch_evo.py
Content:
import argparse
import json
import os
import subprocess
import time
from datetime import datetime

import pandas as pd
import torch

import openai
import wandb


gpt_model = "gpt-4"

API_MAX_RETRY = 16
API_RETRY_SLEEP = 10
API_ERROR_OUTPUT = "$ERROR$"


def init_archive(config):
    if config["B_PARAMS"] == 2:
        fitnesses = [
            4.75,  # DPO
            4.72,  # HINGE
            4.69,  # IPO
            4.68,  # KTO
        ]
    else:
        fitnesses = [
            7.887500,  # DPO
            7.881250,  # HINGE
            7.84,  # IPO
            7.603125,  # KTO
        ]
    archive = []
    archive.append(
        {  # DPO
            "code": """
def logistic_log_loss(
    self,
    policy_chosen_logps: torch.FloatTensor,
    policy_rejected_logps: torch.FloatTensor,
    reference_chosen_logps: torch.FloatTensor,
    reference_rejected_logps: torch.FloatTensor,
) -> torch.FloatTensor:
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps
    logits = pi_logratios - ref_logratios
    losses = -F.logsigmoid(self.beta * logits)
    return losses
    """,
            "fitness": None,
        }
    )
    archive.append(
        {  # HINGE
            "code": """
def hinge_loss(
    self,
    policy_chosen_logps: torch.FloatTensor,
    policy_rejected_logps: torch.FloatTensor,
    reference_chosen_logps: torch.FloatTensor,
    reference_rejected_logps: torch.FloatTensor,
) -> torch.FloatTensor:
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps
    logits = pi_logratios - ref_logratios
    losses = torch.relu(1 - self.beta * logits)
    return losses
    """,
            "fitness": None,
        }
    )
    archive.append(
        {  # IPO
            "code": """
def ipo_loss(
    self,
    policy_chosen_logps: torch.FloatTensor,
    policy_rejected_logps: torch.FloatTensor,
    reference_chosen_logps: torch.FloatTensor,
    reference_rejected_logps: torch.FloatTensor,
) -> torch.FloatTensor:
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps
    logits = pi_logratios - ref_logratios
    losses = (logits - 1 / (2 * self.beta)) ** 2
    return losses
    """,
            "fitness": None,
        }
    )
    archive.append(
        {  # KTO
            "code": """
def kto_pair_loss(
    self,
    policy_chosen_logps: torch.FloatTensor,
    policy_rejected_logps: torch.FloatTensor,
    reference_chosen_logps: torch.FloatTensor,
    reference_rejected_logps: torch.FloatTensor,
) -> torch.FloatTensor:
    chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)
    rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)

    chosen_logratios = policy_chosen_logps - reference_chosen_logps
    rejected_logratios = policy_rejected_logps - reference_rejected_logps
    # As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.
    losses = torch.cat(
        (
            1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)),
            1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios)),
        ),
        0,
    )
    return losses
    """,
            "fitness": None,
        }
    )

    for archive_entry, new_fitness in zip(archive, fitnesses):
        archive_entry["fitness"] = new_fitness

    return archive


def validate_code(code: str) -> bool:
    # Run code through test
    try:
        # Namespace dictionary to hold the execution context
        namespace = {}

        # Execute the function definition string within the provided namespace
        exec(code, globals(), namespace)

        names = list(namespace.keys())
        if len(names) != 1:
            return False, f"{len(names)} things in namespace. Please only provide 1"
        func = namespace[names[0]]
        if not callable(func):
            return False, f"{func} is not callable"

        # Create a class to hold the sigmoid_loss function
        class LossModel:
            def __init__(self, beta):
                self.beta = beta

        # Add the function to the class
        setattr(LossModel, "epo_loss", func)

        model = LossModel(beta=0.05)

        # Define input tensors with requires_grad to check gradients
        policy_chosen_logps = torch.randn(10, requires_grad=True)
        policy_rejected_logps = torch.randn(10, requires_grad=True)
        reference_chosen_logps = torch.randn(10, requires_grad=True)
        reference_rejected_logps = torch.randn(10, requires_grad=True)

        # Compute the loss
        loss = model.epo_loss(
            policy_chosen_logps,
            policy_rejected_logps,
            reference_chosen_logps,
            reference_rejected_logps,
        )

        # Check for NaNs in the output
        if torch.isnan(loss).any():
            return False, "Loss contains NaNs"

        # Check the shape of the output
        if loss.shape != (10,):
            return (
                False,
                f"Expected loss shape to be per input (e.g. (10,)), got {loss.shape}",
            )

        # Backward pass to compute gradients
        loss.mean().backward()

        # Check for NaNs in gradients
        for param in [
            policy_chosen_logps,
            policy_rejected_logps,
            reference_chosen_logps,
            reference_rejected_logps,
        ]:
            if torch.isnan(param.grad).any():
                return False, f"Gradient for {param} contains NaNs"

        return True, ""

    except Exception as e:
        return False, str(e)


def train_gpo(info, config):
    # STORE CODE
    with open(f"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/tests.json", "r") as f:
        tests = json.load(f)
    info["name"] = info["name"].lower().replace(" ", "_")
    for test in tests:  # MAKE NAME UNIQUE.
        if test["name"] == info["name"]:
            info["name"] = f"{info['name']}_{len(tests)}"

    tests.append(
        {
            "name": info["name"],
            "code": info["code"],
        }
    )
    with open(f"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/tests.json", "w") as f:
        json.dump(tests, f, indent=4)
    # Define the command as a list of arguments
    if config["NUM_GPUS"] == 4:
        command = [
            "accelerate",
            "launch",
            "--config_file",
            "recipes/accelerate_configs/deepspeed_zero3.yaml",
            "--num_processes=4",
            "scripts/run_gpo.py",
            f"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/config_full.yaml",
            "--gradient_accumulation_steps=16",
        ]
    else:
        command = [
            "accelerate",
            "launch",
            "--config_file",
            "recipes/accelerate_configs/deepspeed_zero3.yaml",
            "scripts/run_gpo.py",
            f"recipes/zephyr-{config['B_PARAMS']}b-gemma/gpo/config_full.yaml",
        ]

    # Set environment variables directly in the subprocess call
    env = dict(
        os.environ, ACCELERATE_LOG_LEVEL="info"
    )  # Copy current environment and add/modify

    # Execute the command
    result = subprocess.run(command, env=env)
    if result.returncode == 0:
        return True, ""
    return False, f"Failed with return code: {result.returncode}\n{result.stderr}"


def evaluate_gpo(info, config):
    model_id = f"zephyr-{config['B_PARAMS']}b-g-{info['name']}"
    # Command to run the script using the specified Python interpreter
    command = [
        "python",
        "gen_model_answer.py",
        "--model-path",
        f"data/zephyr-{config['B_PARAMS']}b-gemma-{info['name']}",
        "--model-id",
        model_id,
        "--num-gpus-total",
        str(config["NUM_GPUS"]),
    ]

    cwd = config["LLM_JUDGE_DIR"]
    result = subprocess.run(command, cwd=cwd)
    if result.returncode != 0:
        return (
            False,
            f"Gen Model failed with return code: {result.returncode}\n{result.stderr}",
        )

    command = [
        "python",
        "gen_judgment.py",
        "--model-list",
        model_id,
        "--parallel",
        "4",
    ]
    result = subprocess.run(command, cwd=cwd)
    if result.returncode != 0:
        return (
            False,
            f"Gen Judgemnt failed with return code: {result.returncode}\n{result.stderr}",
        )
    input_file = osp.join(cwd, "data/mt_bench/model_judgment/gpt-4_single.jsonl")

    print(f"Input file: {input_file}")
    df_all = pd.read_json(input_file, lines=True)
    df = df_all[["model", "score", "turn"]]
    df = df[df["score"] != -1]
    df = df[df["model"].isin([model_id])]

    print("\n########## First turn ##########")
    df_1 = df[df["turn"] == 1].groupby(["model", "turn"]).mean()
    print(df_1.sort_values(by="score", ascending=False))

    print("\n########## Second turn ##########")
    df_2 = df[df["turn"] == 2].groupby(["model", "turn"]).mean()
    print(df_2.sort_values(by="score", ascending=False))

    print("\n########## Average ##########")
    df_3 = df[["model", "score"]].groupby(["model"]).mean()
    print(df_3.sort_values(by="score", ascending=False))
    model_score = df_3.loc[model_id]["score"]

    if model_score >= 7.9:
        print("UPLOADING")
        command = [
            "huggingface-cli",
            "upload",
            f"zephyr-{config['B_PARAMS']}b-gemma-{info['name']}",
            ".",
            ".",
        ]

        cwd = f"data/zephyr-{config['B_PARAMS']}b-gemma-{info['name']}"
        result = subprocess.run(command, cwd=cwd)
        if result.returncode != 0:
            print("UPLOAD FAILED")
            print(
                f"Upload Model failed with return code: {result.returncode}\n{result.stderr}"
            )

    elif not model_score > 7.75:
        print("DELETING")
        command = [
            "rm",
            "-rf",
            f"data/zephyr-{config['B_PARAMS']}b-gemma-{info['name']}",
        ]
        result = subprocess.run(command)
        if result.returncode != 0:
            print(
                f"Delete Model failed with return code: {result.returncode}\n{result.stderr}"
            )

    return True, model_score


parser = argparse.ArgumentParser()
parser.add_argument("--wandb", action="store_true", default=False)
parser.add_argument("--group", type=str, default="unlabeled")
parser.add_argument("--run-name", type=str, default=None)
parser.add_argument("--num-generations", type=int, default=5)
parser.add_argument("--no-logging", action="store_true", default=False)
parser.add_argument("--num-gpus", type=int, default=8)
parser.add_argument("--b-params", type=int, default=7)
parser.add_argument("--do-baselines", action="store_true", default=False)
parser.add_argument("--resume", type=str, default=None)
parser.add_argument(
    "--llm-judge-dir", type=str, default="../FastChat/fastchat/llm_judge"
)

if __name__ == "__main__":
    args = parser.parse_args()
    assert args.num_gpus in [4, 8], "NUM GPUS must be 4 or 8"
    print(f"NUM GPUS: {args.num_gpus}")
    assert args.b_params in [2, 7], "MUST BE 2B OR 7B"
    print(f"PARAMS: {args.b_params}B")

    now = str(datetime.now()).replace(" ", "_")
    if not args.no_logging:
        save_dir = f"runs/{now}"
        os.mkdir(save_dir)

    config = {
        "NUM_GENERATIONS": args.num_generations,
        "SAVE_DIR": save_dir,
        "NOW": now,
        "NUM_GPUS": args.num_gpus,
        "B_PARAMS": args.b_params,
        "RESUME": args.resume,
        "LLM_JUDGE_DIR": args.llm_judge_dir,
    }
    system_prompt = """
You are a machine learning researcher who is testing out different RLHF loss functions. When you respond, output a JSON where the first key ("thought") corresponds to your thought process when designing the next function. The second key ("name") corresponds to the name of your next function. Finally, the last key ("code") corresponds to the exact python code that you would like to try. Here is an example:

{"thought": "Based on the previous outputs, I should try the direct preference optimization algorithm.",
"name": "dpo",
"code": "def sigmoid_loss(
    self,
    policy_chosen_logps: torch.FloatTensor,
    policy_rejected_logps: torch.FloatTensor,
    reference_chosen_logps: torch.FloatTensor,
    reference_rejected_logps: torch.FloatTensor,
) -> torch.FloatTensor:
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps
    logits = pi_logratios - ref_logratios
    losses = -F.logsigmoid(self.beta * logits)
    return losses"
}

You are deeply familiar with binary classification losses from the literature. Be creative and reference prior literature when possible.

You must use the exact function interface used above. Feel free to define extra hyperparameters within your function as constants. Do not make them attributes of self.

Note that `self.beta = 0.05`.

RLHF loss functions train on a dataset of pairs of preferred and rejected completions.
`policy_chosen_logps` refers to the policy's log probabilities of the preferred completion, and `policy_rejected_logps` refers to the policy's log probabilities of the rejected completion.
`reference_chosen_logps` and `reference_rejected_logps` refer to the same for the reference (base) model.

The user will then return to you a fitness that corresponds to the performance of the resulting model on a downstream task. Your goal is to maximize performance.
"""
    archive = init_archive(config)
    first_prompt = f"""
Here are some results we've obtained: \n{archive}

Please generate the next one.
"""
    if not args.resume:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": first_prompt},
        ]
    else:
        messages = json.load(open(args.resume, "r"))
        assert messages[-1]["role"] == "user", "Last message must be user"

    if not args.no_logging and args.wandb:
        run = wandb.init(
            project="gpo_evo",
            name=args.run_name,
            config=config,
            group=args.group,
        )

    if args.do_baselines:
        baseline_names = ["dpo", "hinge", "ipo", "kto"]
        for archive_entry, name in zip(archive, baseline_names):
            archive_entry["name"] = f"bline-{name}-{args.group}"

    t0 = time.time()
    t_start, t_completion, t_train_start, t_train_end, t_eval_end = t0, t0, t0, t0, t0
    for i in range(args.num_generations):
        if i > 0 and args.wandb and not args.no_logging:
            columns = ["generation", "thought", "function", "fitness", "next prompt"]
            log_table = wandb.Table(columns=columns)
            if "thought" in out and "code" in out:
                log_table.add_data(i, out["thought"], out["code"], fitness, next_prompt)
            else:
                log_table.add_data(i, "", "", fitness, next_prompt)
            wandb.log(
                {
                    "fitness": fitness,
                    "generation": i,
                    "table": log_table,
                    "generation_time": time.time() - t_start,
                    "code_time": t_completion - t_start,
                    "train_time": t_train_end - t_train_start,
                    "eval_time": t_eval_end - t_train_end,
                }
            )
        t_start = time.time()
        # GENERATE CODE
        if not args.do_baselines:
            for _ in range(API_MAX_RETRY):
                try:
                    completion = openai.ChatCompletion.create(
                        engine=gpt_model,
                        messages=messages,
                        max_tokens=2048,
                        n=1,
                        response_format={"type": "json_object"},
                    ).choices[0]
                    break
                except openai.error.OpenAIError as e:
                    print(type(e), e)
                    time.sleep(API_RETRY_SLEEP)
                except openai.error.InvalidRequestError as e:
                    print(type(e), e)
                    break
            t_completion = time.time()

            messages.append(completion.message.to_dict())
            if not args.no_logging:
                with open(f"{save_dir}/messages.json", "w") as f:
                    json.dump(messages, f, indent=4)
            out = json.loads(completion.message.content)
        else:
            out = {"name": archive[i]["name"], "code": archive[i]["code"]}

        # VALIDATE CODE
        valid, error = validate_code(out["code"])
        if not valid:
            next_prompt = (
                f"Code not valid. Error:\n{error}\nPlease generate the next one."
            )
            messages.append({"role": "user", "content": next_prompt})
            fitness = -1
            print("CODE NOT VALID")
            continue
        t_train_start = time.time()

        # TRAIN GPO
        trained, error = train_gpo(out, config)
        if not trained:
            next_prompt = (
                f"Training failed. Error:\n{error}\nPlease generate the next one."
            )
            messages.append({"role": "user", "content": next_prompt})
            fitness = -1
            print("FAILED TRAINING")
            continue
        t_train_end = time.time()

        # EVALUATE GPO
        evaluated, val = evaluate_gpo(out, config)
        if not evaluated:
            next_prompt = (
                f"Evaluation failed. Error:\n{val}\nPlease generate the next one."
            )
            messages.append({"role": "user", "content": next_prompt})
            fitness = -1
            print("FAILED EVAL")
            continue
        t_eval_end = time.time()

        next_prompt = f"Fitness: {val}.\nPlease generate the next one."
        messages.append({"role": "user", "content": next_prompt})
        fitness = val

File Path: scripts/plotting/bar_plot_alpaca_eval.py
Content:
import pandas as pd


def configure_plotting_sn_params(sn, SCALE, HEIGHT_SCALE, use_autolayout=True):
    pd.set_option("mode.chained_assignment", None)
    sn.set(
        rc={
            "figure.figsize": (SCALE, int(HEIGHT_SCALE * SCALE)),
            "figure.autolayout": use_autolayout,
            "text.usetex": True,
            "text.latex.preamble": "\n".join(
                [
                    r"\usepackage{siunitx}",  # i need upright \micro symbols, but you need...
                    r"\sisetup{detect-all}",  # ...this to force siunitx to actually use your fonts
                    r"\usepackage{helvet}",  # set the normal font here
                    r"\usepackage{sansmath}",  # load up the sansmath so that math -> helvet
                    r"\usepackage{amsmath}",
                    r"\sansmath",  # <- tricky! -- gotta actually tell tex to use!
                ]
            ),
        }
    )
    sn.set(font_scale=2.0)
    sn.set_style(
        "white",
        {
            "font.family": "serif",
            "font.serif": "Times New Roman",
            "pdf.fonttype": 42,
            "ps.fonttype": 42,
            "font.size": 14,
        },
    )
    sn.color_palette("colorblind")
    return sn


def plot_alaca_eval_bar_chart():
    import matplotlib.pyplot as plt

    import seaborn as sn

    # plt.rcParams["font.family"] = "Times New Roman"
    # SCALE = 13
    SCALE = 11
    # SCALE = 8
    # HEIGHT_SCALE =0.8
    # HEIGHT_SCALE =0.5
    HEIGHT_SCALE = 1.0
    1 / HEIGHT_SCALE  # -(0.05 + LEGEND_Y_CORD)

    sn = configure_plotting_sn_params(sn, SCALE, HEIGHT_SCALE)
    # plt.gcf().subplots_adjust(bottom=0.40, left=0.2, top=0.95)

    # import matplotlib.pyplot as plt
    # import numpy as np

    # Data for plotting
    # model_names = ['AlphaCode', 'Incoder', 'CodeGeex', 'CodeGeex-Mono', 'PaLM Coder',
    #             'Codex',
    # human_eval_scores = [17.1, 15.2, 17.6, 26.9, 32.9, 38.6, 47.0, 67.7, 65.8, 87.7]

    import seaborn as sns

    # Set the seaborn style
    sns.set_style("whitegrid")

    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap
    from matplotlib.patches import Patch

    import seaborn as sns

    # Define the evaluation scores
    scores = {
        "DPO": 65.29,
        "SLiC": 62.09,
        "DBQL": 61.56,
        "AQL": 63.57,
        "PADLL": 67.20,
        "AQFL": 63.38,
        "CELL": 63.96,
        "LRML - DiscoPOP": 70.83,
        "PFL": 63.00,
    }

    # Sort dictionary by values
    scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=False))

    model_names, eval_scores = [], []
    for model_name, score in scores.items():
        model_names.append(model_name)
        eval_scores.append(score)

    # Choose a color palette
    blue_palette = sns.light_palette("skyblue", reverse=True, n_colors=len(scores))

    # Convert Seaborn color palette to a Matplotlib colormap
    cmap = LinearSegmentedColormap.from_list("custom_cmap", blue_palette)

    # Set the seaborn style
    sns.set_style("whitegrid")

    # Define the figure and axis
    fig, ax = plt.subplots()

    # Identify baseline models
    baseline_models = ["DPO", "SLiC"]

    # Create the horizontal bar chart
    bars = ax.barh(model_names, eval_scores, color=blue_palette, edgecolor="grey")

    # Custom function to add gradient to the bars
    def gradient_bars(bars, cmap, vmin, vmax):
        grad = np.atleast_2d(np.linspace(0, 1, 256))
        ax = bars[0].axes
        lim = ax.get_xlim() + ax.get_ylim()
        for bar in bars:
            bar.set_zorder(1)
            bar.set_facecolor("none")
            x, y = bar.get_xy()
            w, h = bar.get_width(), bar.get_height()
            ax.imshow(
                grad,
                extent=[x, x + w, y, y + h],
                aspect="auto",
                zorder=0,
                cmap=cmap,
                vmin=vmin,
                vmax=vmax,
            )
        ax.axis(lim)

    gradient_bars(bars, cmap, vmin=61, vmax=72)

    # Highlight baseline models with a different style
    for bar, model_name in zip(bars, model_names):
        if model_name in baseline_models:
            bar.set_edgecolor("black")
            bar.set_hatch("//")

    # Remove the spines
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)
    ax.spines["left"].set_visible(False)

    # Add horizontal grid lines only
    ax.xaxis.grid(True, linestyle="--", which="major", color="grey", alpha=0.25)
    ax.yaxis.grid(False)
    ax.set(xlim=(61, 72))

    # Add value labels to the bars
    def add_labels(bars):
        for bar in bars:
            width = bar.get_width()
            ax.annotate(
                f"{width:.2f}%",
                xy=(width, bar.get_y() + bar.get_height() / 2),
                xytext=(3, 0),  # 3 points horizontal offset
                textcoords="offset points",
                ha="left",
                va="center",
            )

    add_labels(bars)

    # Set labels and title
    plt.xlabel("Win Rate - LC (\%)")
    plt.title("Held Out Alpaca Eval Performance")

    # Create legend
    handles = [
        Patch(color="skyblue", label="Discovered"),
        Patch(facecolor="skyblue", edgecolor="black", hatch="//", label="Baselines"),
    ]
    ax.legend(handles=handles, title="Model Type", loc="lower right")

    # Set background color
    ax.set_facecolor("white")
    fig.patch.set_facecolor("white")

    # Adjust layout
    plt.tight_layout()
    plt.savefig("./plots/alpaca_eval_bar_horizontal.png")
    plt.savefig("./plots/alpaca_eval_bar_horizontal.pdf")
    print("./plots/alpaca_eval_bar_horizontal.png")
    plt.clf()


if __name__ == "__main__":
    plot_alaca_eval_bar_chart()

File Path: scripts/plotting/plot_beta_sweep.py
Content:
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F

import seaborn as sn
from bar_plot_alpaca_eval import configure_plotting_sn_params


# plt.rcParams["font.family"] = "Times New Roman"
# SCALE = 13
SCALE = 20
# SCALE = 20
# SCALE = 8
# HEIGHT_SCALE =0.8
# HEIGHT_SCALE =0.5
HEIGHT_SCALE = 1
LEGEND_Y_CORD = -0.75 * (HEIGHT_SCALE / 2.0)
SUBPLOT_ADJUST = 1 / HEIGHT_SCALE  # -(0.05 + LEGEND_Y_CORD)
LEGEND_X_CORD = 0.45
PLOT_FROM_CACHE = False
PLOT_SAFTEY_MARGIN = 1.25
MODEL_NAME_MAP = {}

sn = configure_plotting_sn_params(sn, SCALE, HEIGHT_SCALE)

# import matplotlib.pyplot as plt
# import numpy as np

# Data for plotting
# model_names = ['AlphaCode', 'Incoder', 'CodeGeex', 'CodeGeex-Mono', 'PaLM Coder',
#             'Codex',
# human_eval_scores = [17.1, 15.2, 17.6, 26.9, 32.9, 38.6, 47.0, 67.7, 65.8, 87.7]

import seaborn as sns


# Set the seaborn style
sns.set_style("whitegrid")


# Define colors for each loss
num_dpo_hinge = 2
num_other = 7  # Number of other loss types to plot

dpo_hinge_colors = plt.cm.Blues(np.linspace(0.5, 0.9, num_dpo_hinge))  # Shades of blue for dpo and hinge
other_colors = plt.cm.Reds(np.linspace(0.2, 1, num_other))  # Shades of red for other losses

loss_colors = {
    "dpo": dpo_hinge_colors[0],
    "hinge": dpo_hinge_colors[1],
    "dbaql": other_colors[0],
    "aql": other_colors[1],
    "padll": other_colors[4],
    "aqfl": other_colors[2],
    "cell": other_colors[4],
    "lrml": other_colors[6],
    "pfl": other_colors[5],
}

loss_names = {
    "dpo": "DPO",
    "hinge": "SLiC",
    "dbaql": "DBAQL",
    "aql": "AQL",
    "padll": "PADLL",
    "aqfl": "AQFL",
    "cell": "CELL",
    "lrml": "LRML",
    "pfl": "PFL",
}


def log_ratio_modulated_loss(logits_in: torch.Tensor, beta, tau=0.05) -> torch.FloatTensor:
    logits = beta * logits_in
    # Modulate the mixing coefficient based on the log ratio magnitudes
    log_ratio_modulation = torch.sigmoid(logits / tau)
    logistic_component = -F.logsigmoid(logits)
    exp_component = torch.exp(-logits)
    # Blend between logistic and exponential component based on log ratio modulation
    losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation
    return losses


def logistic_log_loss(logits_in, beta) -> torch.FloatTensor:
    logits = beta * logits_in
    losses = -F.logsigmoid(logits)
    return losses


# Generate logit values from -10 to 10
logits = torch.arange(-20, 40, 0.01, dtype=torch.float)

# Calculate losses for different beta values
betas = [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5]
losses = [log_ratio_modulated_loss(logits, beta) for beta in betas]
losses_dpo = [logistic_log_loss(logits, beta) for beta in betas]


# Create a 3x3 subfigure plot
fig, axs = plt.subplots(3, 3, figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))
fig.suptitle(r"Comparison of DPO vs LRML for different $\boldsymbol{\beta}$ Values")

# Plot each loss in the grid
for i, (ax, beta, loss, loss_dpo) in enumerate(zip(axs.flat, betas, losses, losses_dpo)):
    ax.plot(logits, loss_dpo, label="DPO", color=loss_colors["dpo"])
    ax.plot(logits, loss, label="LRML", color=loss_colors["lrml"])
    ax.set_xlabel(r"Logits $\boldsymbol{\rho}$")
    ax.set_ylabel(r"Loss $f(\boldsymbol{\rho})$")
    ax.set_title(r"$\boldsymbol{\beta} =$" + f"{beta}")
    ax.legend()

# Add labels and title
plt.savefig("./plots/losses_sweep.pdf", bbox_inches="tight")
plt.savefig("./plots/losses_sweep.png", bbox_inches="tight")
print("./plots/losses_sweep.png")


print("Done")
plt.clf()


##################### Gradients

logits = torch.arange(-20, 40, 0.01, dtype=torch.float, requires_grad=True)

# Calculate losses for different beta values
betas = [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5]

losses_gradients = []
losses_dpo_gradients = []

for beta in betas:
    logits.grad = None  # Reset gradients
    loss = log_ratio_modulated_loss(logits, beta).sum()
    loss.backward()
    losses_gradients.append(logits.grad.clone().detach().numpy())

    logits.grad = None  # Reset gradients
    loss_dpo = logistic_log_loss(logits, beta).sum()
    loss_dpo.backward()
    losses_dpo_gradients.append(logits.grad.clone().detach().numpy())

fig, axs = plt.subplots(3, 3, figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))
fig.suptitle(r"Comparison of DPO vsLRML Gradients for Different $\boldsymbol{\beta}$ Values")

for i, (ax, beta, loss_grad, loss_dpo_grad) in enumerate(zip(axs.flat, betas, losses_gradients, losses_dpo_gradients)):
    ax.plot(logits.detach().numpy(), loss_dpo_grad, label="DPO", color=loss_colors["dpo"])
    ax.plot(logits.detach().numpy(), loss_grad, label="LRML", color=loss_colors["lrml"])
    ax.set_xlabel(r"Logits $\boldsymbol{\rho}$")
    ax.set_ylabel(r"Loss Gradient $\boldsymbol{\nabla}_{\rho}f(\boldsymbol{\rho})$")
    ax.set_title(r"$\boldsymbol{\beta} =$" + f"{beta}")
    ax.legend()

plt.tight_layout(rect=[0, 0, 1, 0.96])

plt.savefig("./plots/gradients_sweep.pdf", bbox_inches="tight")
plt.savefig("./plots/gradients_sweep.png", bbox_inches="tight")
print("./plots/gradients_sweep.png")

print("Done")
plt.clf()

File Path: scripts/plotting/plot_imdb.py
Content:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import seaborn as sn
from bar_plot_alpaca_eval import configure_plotting_sn_params


# plt.rcParams["font.family"] = "Times New Roman"
# SCALE = 13
SCALE = 10.5
# SCALE = 20
# SCALE = 8
# HEIGHT_SCALE =0.8
# HEIGHT_SCALE =0.5
HEIGHT_SCALE = 0.8
LEGEND_Y_CORD = -0.75 * (HEIGHT_SCALE / 2.0)
SUBPLOT_ADJUST = 1 / HEIGHT_SCALE  # -(0.05 + LEGEND_Y_CORD)
LEGEND_X_CORD = 0.45
PLOT_FROM_CACHE = False
PLOT_SAFTEY_MARGIN = 1.25
MODEL_NAME_MAP = {}

sn = configure_plotting_sn_params(sn, SCALE, HEIGHT_SCALE)
plt.gcf().subplots_adjust(bottom=0.40, left=0.2, top=0.95)

# import matplotlib.pyplot as plt
# import numpy as np

# Data for plotting
# model_names = ['AlphaCode', 'Incoder', 'CodeGeex', 'CodeGeex-Mono', 'PaLM Coder',
#             'Codex',
# human_eval_scores = [17.1, 15.2, 17.6, 26.9, 32.9, 38.6, 47.0, 67.7, 65.8, 87.7]

import seaborn as sns


# Set the seaborn style
sns.set_style("whitegrid")


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))


# Read DataFrame
df = pd.read_csv("./scripts/plotting/imdb_results_2.csv")
defined_betas = [0.025, 0.05, 0.1, 0.25, 0.5, 1.0]

# Define colors for each loss
num_dpo_hinge = 2
num_other = 7  # Number of other loss types to plot

dpo_hinge_colors = plt.cm.Blues(np.linspace(0.5, 0.9, num_dpo_hinge))  # Shades of blue for dpo and hinge
other_colors = plt.cm.Reds(np.linspace(0.2, 1, num_other))  # Shades of red for other losses

loss_colors = {
    "dpo": dpo_hinge_colors[0],
    "hinge": dpo_hinge_colors[1],
    "dbaql": other_colors[0],
    "aql": other_colors[1],
    "padll": other_colors[2],
    "aqfl": other_colors[4],
    "cell": other_colors[4],
    "lrml": other_colors[6],
    "pfl": other_colors[6],
}

loss_names = {
    "dpo": "DPO",
    "hinge": "SLiC",
    "dbaql": "DBAQL",
    "aql": "AQL",
    "padll": "PADLL",
    "aqfl": "AQFL",
    "cell": "CELL",
    "lrml": "LRML",
    "pfl": "PFL",
}

# Filter the DataFrame to include only rows with beta values in defined_betas
df_filtered = df[df["beta"].isin(defined_betas)]

# Group by 'loss' and 'beta' and calculate mean and std of 'kl_divergence' and 'reward'
mean_df = (
    df_filtered.groupby(["loss", "beta"])
    .agg({"kl_divergence": ["mean", "std"], "reward": ["mean", "std"]})
    .reset_index()
)

# Flatten the MultiIndex columns
mean_df.columns = [
    "loss",
    "beta",
    "kl_divergence_mean",
    "kl_divergence_std",
    "reward_mean",
    "reward_std",
]

for loss in ["dpo", "lrml"]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )

    # Annotate each point with the corresponding beta value
    # Annotate each point with the corresponding beta value
    for i, row in loss_df.iterrows():
        if loss == "lrml":
            offset_x, offset_y = -0.001, 0.001  # Adjust these values as needed
            ha, va = "right", "bottom"
        else:
            offset_x, offset_y = 0.005, -0.001  # Adjust these values as needed
            ha, va = "left", "top"

        plt.text(
            row["kl_divergence_mean"] + offset_x,
            row["reward_mean"] + offset_y,
            r"$\boldsymbol{\beta} =$" + f"{row['beta']}",
            fontsize=18,
            ha=ha,
            va=va,
            color=loss_colors[loss],
        )

# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation: DPO vs LRML")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_dpo_lrml.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_dpo_lrml.png", bbox_inches="tight")
print("./plots/imdb_dpo_lrml.png")


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

mean_df.columns = [
    "loss",
    "beta",
    "kl_divergence_mean",
    "kl_divergence_std",
    "reward_mean",
    "reward_std",
]

for loss in ["hinge", "lrml"]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )

    # Annotate each point with the corresponding beta value
    # Annotate each point with the corresponding beta value
    for i, row in loss_df.iterrows():
        if loss == "lrml":
            offset_x, offset_y = -0.001, 0.001  # Adjust these values as needed
            ha, va = "right", "bottom"
        else:
            offset_x, offset_y = 0.005, -0.001  # Adjust these values as needed
            ha, va = "left", "top"

        plt.text(
            row["kl_divergence_mean"] + offset_x,
            row["reward_mean"] + offset_y,
            r"$\boldsymbol{\beta} =$" + f"{row['beta']}",
            fontsize=18,
            ha=ha,
            va=va,
            color=loss_colors[loss],
        )

# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation: SLiC vs LRML")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_slic_lrml.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_slic_lrml.png", bbox_inches="tight")
print("./plots/imdb_slic_lrml.png")


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

for loss in ["dpo", "padll"]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )

    # Annotate each point with the corresponding beta value
    # Annotate each point with the corresponding beta value
    for i, row in loss_df.iterrows():
        if loss == "padll":
            offset_x, offset_y = -0.001, 0.001  # Adjust these values as needed
            ha, va = "right", "bottom"
        else:
            offset_x, offset_y = 0.005, -0.001  # Adjust these values as needed
            ha, va = "left", "top"

        plt.text(
            row["kl_divergence_mean"] + offset_x,
            row["reward_mean"] + offset_y,
            r"$\boldsymbol{\beta} =$" + f"{row['beta']}",
            fontsize=18,
            ha=ha,
            va=va,
            color=loss_colors[loss],
        )

# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation: DPO vs PADLL")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_dpo_padll.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_dpo_padll.png", bbox_inches="tight")
print("./plots/imdb_dpo_padll.png")


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

for loss in ["hinge", "padll"]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )

    # Annotate each point with the corresponding beta value
    # Annotate each point with the corresponding beta value
    for i, row in loss_df.iterrows():
        if loss == "padll":
            offset_x, offset_y = -0.001, 0.001  # Adjust these values as needed
            ha, va = "right", "bottom"
        else:
            offset_x, offset_y = 0.005, -0.001  # Adjust these values as needed
            ha, va = "left", "top"

        plt.text(
            row["kl_divergence_mean"] + offset_x,
            row["reward_mean"] + offset_y,
            r"$\boldsymbol{\beta} =$" + f"{row['beta']}",
            fontsize=18,
            ha=ha,
            va=va,
            color=loss_colors[loss],
        )

# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation: SLiC vs PADLL")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_slic_padll.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_slic_padll.png", bbox_inches="tight")
print("./plots/imdb_slic_padll.png")


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

for loss in ["dpo", "aqfl"]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )

    # Annotate each point with the corresponding beta value
    # Annotate each point with the corresponding beta value
    for i, row in loss_df.iterrows():
        if loss == "aqfl":
            offset_x, offset_y = -0.001, 0.001  # Adjust these values as needed
            ha, va = "right", "bottom"
        else:
            offset_x, offset_y = 0.005, -0.001  # Adjust these values as needed
            ha, va = "left", "top"

        plt.text(
            row["kl_divergence_mean"] + offset_x,
            row["reward_mean"] + offset_y,
            r"$\boldsymbol{\beta} =$" + f"{row['beta']}",
            fontsize=18,
            ha=ha,
            va=va,
            color=loss_colors[loss],
        )

# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation: DPO vs AQFL")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_dpo_aqfl.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_dpo_aqfl.png", bbox_inches="tight")
print("./plots/imdb_dpo_aqfl.png")


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

for loss in ["hinge", "aqfl"]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )

    # Annotate each point with the corresponding beta value
    # Annotate each point with the corresponding beta value
    for i, row in loss_df.iterrows():
        if loss == "aqfl":
            offset_x, offset_y = -0.001, 0.001  # Adjust these values as needed
            ha, va = "right", "bottom"
        else:
            offset_x, offset_y = 0.005, -0.001  # Adjust these values as needed
            ha, va = "left", "top"

        plt.text(
            row["kl_divergence_mean"] + offset_x,
            row["reward_mean"] + offset_y,
            r"$\boldsymbol{\beta} =$" + f"{row['beta']}",
            fontsize=18,
            ha=ha,
            va=va,
            color=loss_colors[loss],
        )

# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation: SLiC vs AQFL")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_slic_aqfl.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_slic_aqfl.png", bbox_inches="tight")
print("./plots/imdb_slic_aqfl.png")


# plt.rcParams["font.family"] = "Times New Roman"
# SCALE = 13
SCALE = 10.5
# SCALE = 20
# SCALE = 8
# HEIGHT_SCALE =0.8
# HEIGHT_SCALE =0.5
HEIGHT_SCALE = 0.8
LEGEND_Y_CORD = -(HEIGHT_SCALE / 2.0)
SUBPLOT_ADJUST = 1 / HEIGHT_SCALE  # -(0.05 + LEGEND_Y_CORD)
LEGEND_X_CORD = 0.45
PLOT_FROM_CACHE = False
PLOT_SAFTEY_MARGIN = 1.25
MODEL_NAME_MAP = {}


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

for loss in [
    "dpo",
    "hinge",
    "aqfl",
    "padll",
    "lrml",
]:
    loss_df = mean_df[mean_df["loss"] == loss]
    plt.errorbar(
        loss_df["kl_divergence_mean"],
        loss_df["reward_mean"],
        # xerr=row['kl_divergence_std'],
        # yerr=row['reward_std'],
        label=loss_names[loss],
        color=loss_colors[loss],
        linestyle=":",
        marker="o",
        ms=9,
        lw=2.5,
    )


# Add labels and title
plt.xlabel("KL Divergence")
plt.ylabel("Reward")
plt.title("IMDb Positive Text Generation")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=4,
    fancybox=True,
    shadow=True,
)
plt.xlim(0, 1.5)
plt.savefig("./plots/imdb_all.pdf", bbox_inches="tight")
plt.savefig("./plots/imdb_all.png", bbox_inches="tight")
print("./plots/imdb_all.png")


# # Define a list of markers to use
# # SCALE = 13
# SCALE = 10.5
# # SCALE = 20
# # SCALE = 8
# # HEIGHT_SCALE =0.8
# # HEIGHT_SCALE =0.5
# HEIGHT_SCALE =0.8
# LEGEND_Y_CORD = - 0.75* (HEIGHT_SCALE / 2.0)
# SUBPLOT_ADJUST = 1 / HEIGHT_SCALE  # -(0.05 + LEGEND_Y_CORD)
# LEGEND_X_CORD = 0.45
# PLOT_FROM_CACHE = False
# PLOT_SAFTEY_MARGIN = 1.25
# MODEL_NAME_MAP = {}

# sn = configure_plotting_sn_params(sn, SCALE, HEIGHT_SCALE)
# plt.gcf().subplots_adjust(bottom=0.1, left=0.2, top=0.95)

# # import matplotlib.pyplot as plt
# # import numpy as np

# # Data for plotting
# # model_names = ['AlphaCode', 'Incoder', 'CodeGeex', 'CodeGeex-Mono', 'PaLM Coder',
# #             'Codex',
# # human_eval_scores = [17.1, 15.2, 17.6, 26.9, 32.9, 38.6, 47.0, 67.7, 65.8, 87.7]
# from matplotlib import cm
# import seaborn as sns


# # Set the seaborn style
# sns.set_style("whitegrid")


# markers = ['o', 's', '^', 'D', '*', 'p', 'h', 'p', '*', 'h']

# plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

# # Plot lines for each loss method
# for loss in ["aqfl", "padll", "lrml", "dpo", "hinge"]:
#     loss_df = mean_df[mean_df['loss'] == loss]
#     plt.plot(loss_df['kl_divergence_mean'],
#              loss_df['reward_mean'],
#              label=loss_names[loss],
#              color=loss_colors[loss],
#              linestyle=":",
#              lw=2.5)

# # Plot markers for each beta
# used_markers = {}
# for loss in ["aqfl", "padll", "lrml", "dpo", "hinge"]:
#     loss_df = mean_df[mean_df['loss'] == loss]
#     for i, (index, row) in enumerate(loss_df.iterrows()):
#         marker = markers[i % len(markers)]
#         plt.scatter(row['kl_divergence_mean'],
#                     row['reward_mean'],
#                     color=loss_colors[loss],
#                     marker=marker, s=100)
#         # Add to the used_markers dict for legend
#         if row['beta'] not in used_markers:
#             used_markers[row['beta']] = marker

# # Add labels and title
# plt.xlabel('KL Divergence')
# plt.ylabel('Reward')
# plt.title('IMDb Positive Text Generation: All')

# # Create legends
# # Legend for colors (methods)
# first_legend = plt.legend(loc="lower center", bbox_to_anchor=(
#             LEGEND_X_CORD, LEGEND_Y_CORD), ncol=3, fancybox=True, shadow=True)

# # Legend for markers (betas)
# marker_handles = [plt.Line2D([0], [0], marker=m, color='w', label=r'$\boldsymbol{\beta}=$' + f'{b}',
#                              markerfacecolor='black', markersize=9)
#                   for b, m in used_markers.items()]
# plt.legend(handles=marker_handles, title=r'$\boldsymbol\beta$ Values', loc='lower right', fancybox=True, shadow=True)

# # Add the first legend back
# plt.gca().add_artist(first_legend)

# plt.xlim(0, 1.5)
# plt.savefig(f'./plots/imdb_all.pdf')
# plt.savefig(f'./plots/imdb_all.png')
# print(f'./plots/imdb_all.png')


# print('Done')
# plt.clf()

File Path: scripts/plotting/plot_losses.py
Content:
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F

import seaborn as sn
from bar_plot_alpaca_eval import configure_plotting_sn_params


# plt.rcParams["font.family"] = "Times New Roman"
# SCALE = 13
SCALE = 11
# SCALE = 20
# SCALE = 8
# HEIGHT_SCALE =0.8
# HEIGHT_SCALE =0.5
HEIGHT_SCALE = 0.8
LEGEND_Y_CORD = -1.2 * (HEIGHT_SCALE / 2.0)
SUBPLOT_ADJUST = 1 / HEIGHT_SCALE  # -(0.05 + LEGEND_Y_CORD)
LEGEND_X_CORD = 0.45
PLOT_FROM_CACHE = False
PLOT_SAFTEY_MARGIN = 1.25
MODEL_NAME_MAP = {}

dpo_hinge_colors = plt.cm.Blues(np.linspace(0.5, 0.9, 2))  # Shades of blue for dpo and hinge
other_colors = plt.cm.Reds(np.linspace(0.2, 1, 7))  # Shades of red for other losses

loss_colors = {
    "dpo": dpo_hinge_colors[0],
    "hinge": dpo_hinge_colors[1],
    "dbaql": other_colors[0],
    "aql": other_colors[1],
    "padll": other_colors[4],
    "aqfl": other_colors[2],
    "cell": other_colors[4],
    "lrml": other_colors[6],
    "pfl": other_colors[5],
}

loss_names = {
    "dpo": "DPO",
    "hinge": "SLiC",
    "dbaql": "DBAQL",
    "aql": "AQL",
    "padll": "PADLL",
    "aqfl": "AQFL",
    "cell": "CELL",
    "lrml": "LRML",
    "pfl": "PFL",
}

sn = configure_plotting_sn_params(sn, SCALE, HEIGHT_SCALE)
plt.gcf().subplots_adjust(bottom=0.40, left=0.2, top=0.95)

# import matplotlib.pyplot as plt
# import numpy as np

# Data for plotting
# model_names = ['AlphaCode', 'Incoder', 'CodeGeex', 'CodeGeex-Mono', 'PaLM Coder',
#             'Codex',
# human_eval_scores = [17.1, 15.2, 17.6, 26.9, 32.9, 38.6, 47.0, 67.7, 65.8, 87.7]

import seaborn as sns


# Set the seaborn style
sns.set_style("whitegrid")


beta = 0.05


def performance_adaptive_decay_logistic_loss(logits: torch.Tensor) -> torch.FloatTensor:
    base_decay = 0.9
    mismatch_penalty = 0.5  # Penalty decay for mismatched choices
    mismatches = (logits < 0).float()  # Identify mismatches

    adaptive_decay = base_decay * (1 - mismatches * mismatch_penalty)
    weighted_losses = adaptive_decay * -F.logsigmoid(beta * logits)
    return weighted_losses


def logistic_log_loss(logits) -> torch.FloatTensor:
    losses = -F.logsigmoid(beta * logits)
    return losses


def ipo_loss(logits) -> torch.FloatTensor:
    losses = (logits - 1 / (2 * 0.1)) ** 2
    return losses


def hinge_loss(logits) -> torch.FloatTensor:
    losses = torch.relu(1 - beta * logits)
    return losses


def adaptive_quantile_loss(logits: torch.Tensor) -> torch.FloatTensor:
    percentile = 0.5  # Start with the median quantile
    moving_quantile_weight = 0.01  # Weight for updating the moving quantile

    moving_quantile = percentile + moving_quantile_weight * (torch.sigmoid(logits.mean()) - percentile)

    quantile_weights = torch.sigmoid(-beta * (logits - moving_quantile))

    logistic_losses = -F.logsigmoid(beta * logits)
    hinge_losses = torch.relu(1 - beta * logits)

    # Blend the logistic and hinge losses based on the dynamic quantile weight
    losses = quantile_weights * logistic_losses + (1 - quantile_weights) * hinge_losses
    return losses


def combined_exp_logistic_loss(logits: torch.Tensor) -> torch.FloatTensor:
    exp_losses = torch.exp(-beta * logits)
    log_losses = -F.logsigmoid(beta * logits)
    # Combine the losses with a tunable mixing coefficient
    alpha = 0.5
    losses = alpha * exp_losses + (1 - alpha) * log_losses
    return losses


def log_ratio_modulated_loss(logits: torch.Tensor) -> torch.FloatTensor:
    # Modulate the mixing coefficient based on the log ratio magnitudes
    log_ratio_modulation = torch.sigmoid(logits)
    logistic_component = -F.logsigmoid(beta * logits)
    exp_component = torch.exp(-beta * logits)
    # Blend between logistic and exponential component based on log ratio modulation
    losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation
    return losses


def policy_focused_loss(logits: torch.Tensor) -> torch.FloatTensor:
    focus_scale = 2.0  # Scale to emphasize or de-emphasize based on the correctness of predictions
    is_correct = logits > 0

    logistic_losses = -F.logsigmoid(logits)
    hinge_losses = torch.relu(1 - logits)

    focused_loss = torch.where(
        is_correct,
        logistic_losses / focus_scale,  # De-emphasize correct predictions
        hinge_losses * focus_scale,  # Emphasize incorrect predictions
    )
    return focused_loss


def dynamic_blended_adaptive_quantile_loss(logits) -> torch.FloatTensor:
    import torch.nn.functional as F

    # Constants for the loss function
    starting_quantile = 0.5
    quantile_adapt_rate = 0.01
    temperature = 0.9
    dynamic_blend_rate = 1.0

    logits_variability = logits.var()

    # Calculate an adaptive quantile based on a moving target
    starting_quantile + quantile_adapt_rate * (torch.sigmoid(logits.mean()) - starting_quantile)

    # Calculate dynamic blending coefficient based on logits variability
    dynamic_blend_coeff = torch.sigmoid(logits_variability) * dynamic_blend_rate

    # Prepare components of the blended loss
    logistic_loss = -F.logsigmoid(beta * logits / temperature)
    exp_loss = torch.exp(beta * logits * temperature)

    # Blend the losses dynamically
    losses = dynamic_blend_coeff * logistic_loss + (1 - dynamic_blend_coeff) * exp_loss
    return losses


def adaptive_quantile_feedback_loss(logits) -> torch.FloatTensor:
    import torch.nn.functional as F

    quantile_update_rate = 0.05
    distance_scale = 0.1

    logits_std = logits.std()

    adaptive_quantile = logits_std * torch.sigmoid(-logits).mean()
    adaptive_quantile += quantile_update_rate * (torch.sigmoid(logits.mean()) - adaptive_quantile)

    distance_from_quantile = (logits - adaptive_quantile).abs()
    blend_rate = torch.sigmoid(distance_scale * distance_from_quantile)

    logistic_losses = -F.logsigmoid(beta * logits)
    hinge_losses = torch.relu(1 - beta * logits)

    losses = blend_rate * logistic_losses + (1 - blend_rate) * hinge_losses
    return losses


# Generate logit values from -10 to 10
logits = torch.arange(-10, 40, 0.01, dtype=torch.float)

# Calculate losses for both functions
loss1 = logistic_log_loss(logits)
loss2 = ipo_loss(logits)
loss3 = hinge_loss(logits)
loss4 = performance_adaptive_decay_logistic_loss(logits)
loss5 = adaptive_quantile_loss(logits)
loss6 = combined_exp_logistic_loss(logits)
loss7 = log_ratio_modulated_loss(logits)
loss8 = policy_focused_loss(logits)
loss9 = dynamic_blended_adaptive_quantile_loss(logits)
loss10 = adaptive_quantile_feedback_loss(logits)

# Define the colors for each loss function

# dpo_hinge_colors = plt.cm.Blues(np.linspace(0.5, 0.9, 2))  # Shades of blue for dpo and hinge
# other_colors = plt.cm.Reds(np.linspace(0.5, 0.9, 3))  # Shades of red for other losses

# loss_colors = {
#     "dpo": dpo_hinge_colors[0],
#     "hinge": dpo_hinge_colors[1],
#     "aql": other_colors[0],
#     "padll": other_colors[1],
#     "cell": "gray",  # Not plotted
#     "lrml": other_colors[2],
#     "pfl": "gray"    # Not plotted
# }


plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

# Plot the results
# plt.figure()
# plt.plot(logits, loss1, label='DPO', color= dpo_hinge_colors[0])
# #plt.plot(logits, loss2, label='IPO')
# plt.plot(logits, loss3, label='SLiC', color= dpo_hinge_colors[1])
# plt.plot(logits, loss5, label='Adaptive Quantile Loss', color= other_colors[0],)
# plt.plot(logits, loss4, label='Performance Adaptive Decay Log Loss', color= other_colors[1],)
# plt.plot(logits, loss7, label='Log Ratio Modulated Loss', color= other_colors[2],)
# #plt.plot(logits, loss9, label='New', color= other_colors[2],)
# #plt.plot(logits, loss8, label='Policy Focused Loss')

# plt.plot(logits, loss5, label='AQL')
plt.plot(logits, loss10, label="AQFL", color=loss_colors["aqfl"])
plt.plot(logits, loss4, label="PADLL", color=loss_colors["padll"])
plt.plot(logits, loss7, label="LRML", color=loss_colors["lrml"])
plt.plot(logits, loss1, label="DPO", color=loss_colors["dpo"])
# plt.plot(logits, loss2, label='IPO')
plt.plot(logits, loss3, label="SLiC", color=loss_colors["hinge"])
# plt.plot(logits, loss9, label='New', color= other_colors[2],)
# plt.plot(logits, loss8, label='Policy Focused Loss')
plt.xlabel(r"Logits $\rho$")
plt.ylabel(r"Loss $f(\rho)$")
plt.title("Discovered Objective Functions")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.tight_layout()
plt.savefig("./plots/losses_function.png", bbox_inches="tight")
plt.savefig("./plots/losses_function.pdf", bbox_inches="tight")
print("./plots/losses_function.png")
# plt.grid(True)
# plt.savefig("loss_new.pdf",bbox_inches='tight')
# plt.show()
print("Done")
plt.clf()


# Generate logit values from -10 to 10
logits = torch.arange(-10, 40, 0.01, dtype=torch.float, requires_grad=True)

# Calculate losses for each function
loss_functions = [
    ("aqfl", adaptive_quantile_feedback_loss),
    ("padll", performance_adaptive_decay_logistic_loss),
    ("lrml", log_ratio_modulated_loss),
    ("dpo", logistic_log_loss),
    ("hinge", hinge_loss),
]

# Colors for the gradients
dpo_hinge_colors = plt.cm.Blues(np.linspace(0.5, 0.9, 2))  # Shades of blue for DPO and SLiC
other_colors = plt.cm.Reds(np.linspace(0.5, 0.9, 3))  # Shades of red for other losses

# Plot the gradients
plt.figure(figsize=(SCALE, int(HEIGHT_SCALE * SCALE)))

# Define color mapping for each loss function
# color_mapping = {
#     'DPO': dpo_hinge_colors[0],
#     'SLiC': dpo_hinge_colors[1],
#     #'Adaptive Quantile Loss': other_colors[0],
#     'PADLL': other_colors[0],
#     'LRML': other_colors[2]
# }

for name, loss_fn in loss_functions:
    logits.grad = None  # Clear any existing gradients
    loss = loss_fn(logits)
    loss.backward(torch.ones_like(logits))  # Compute the gradients
    grad = logits.grad.clone().detach().numpy()  # Extract the gradients
    # plt.plot(logits.detach().numpy(), grad, label=name, color=color_mapping[name])
    plt.plot(logits.detach().numpy(), grad, label=loss_names[name], color=loss_colors[name])

plt.xlabel(r"Logits $\rho$")
plt.ylabel(r"Gradient $f'(\rho)$")
plt.title(r"Gradient of Objective Functions")
plt.legend(
    loc="lower center",
    bbox_to_anchor=(LEGEND_X_CORD, LEGEND_Y_CORD),
    ncol=2,
    fancybox=True,
    shadow=True,
)
plt.tight_layout()
plt.savefig("./plots/losses_gradient.png", bbox_inches="tight")
plt.savefig("./plots/losses_gradient.pdf", bbox_inches="tight")
print("./plots/losses_gradient.png")
# plt.grid(True)
# plt.savefig("loss_new.pdf",bbox_inches='tight')
# plt.show()
print("Done")
plt.clf()

File Path: scripts/run_alpaca_eval.py
Content:
import argparse
import os
import subprocess
from datetime import datetime

import numpy as np
import pandas as pd


def evaluate_gpo(model_id, alpaca_eval=False, alpaca_configs=None):
    alpaca_score = np.nan
    if alpaca_eval:
        print("Running Alpaca Eval")
        assert (
            alpaca_configs is not None
        ), "If you wan to evaluate on Alpaca Eval, you need to provide the alpaca configs"

        os.environ["OPENAI_CLIENT_CONFIG_PATH"] = alpaca_configs["openai_configs"]

        # Run Alpaca Eval script
        if alpaca_configs["from_outputs"]:
            if alpaca_configs["reference_model"] == "":
                command = [
                    "alpaca_eval",
                    "evaluate",
                    "--model_outputs",
                    alpaca_configs["model"],
                    "--output_path",
                    f"alpaca_eval/results_outputs_AE_gpt/{model_id}",
                ]
            else:
                command = [
                    "alpaca_eval",
                    "evaluate",
                    "--model_outputs",
                    alpaca_configs["model"],
                    "--reference_outputs",
                    alpaca_configs["reference_model"],
                    "--output_path",
                    f"alpaca_eval/results_from_outputs_AE2/{model_id}",
                ]

        else:
            command = [
                "alpaca_eval",
                "evaluate_from_model",
                "--model_configs",
                alpaca_configs["model"],
                "--reference_model_configs",
                alpaca_configs["reference_model"],
                "--output_path",
                f"alpaca_eval/results/{model_id}",
            ]
        result = subprocess.run(command)
        if result.returncode != 0:
            return (
                False,
                f"Alpaca Eval 2 failed with return code: {result.returncode}\n{result.stderr}",
            )

        if alpaca_configs["from_outputs"]:
            df_alpaca = pd.read_csv(
                f"alpaca_eval/results_from_outputs_AE2/{model_id}/weighted_alpaca_eval_gpt4_turbo/leaderboard.csv",
                index_col="Unnamed: 0",
            )
        else:
            df_alpaca = pd.read_csv(
                f"alpaca_eval/results/{model_id}/alpaca_eval_gpt4/leaderboard.csv",
                index_col="Unnamed: 0",
            )
        print("\n########## ALPACA EVAL ##########")
        alpaca_score = df_alpaca.loc[model_id, "win_rate"]
        print(f"{df_alpaca.loc[model_id, 'win_rate']:.3f} +- {df_alpaca.loc[model_id, 'standard_error']:.3f}")

    return True, alpaca_score


parser = argparse.ArgumentParser()
parser.add_argument(
    "--num-generations",
    type=int,
    default=5,
    help="Number of times the evaluation is to run",
)
parser.add_argument(
    "--model-id",
    type=str,
    help="Your personal model ID, does not have to match the model path name",
)
parser.add_argument("--alpaca-eval", action="store_true", help="Whether to evaluate on Alpaca Eval 2.0")
parser.add_argument("--no-logging", action="store_true")
parser.add_argument("--csv-save-path", type=str, default="./", help="Path to save the CSV file")
parser.add_argument(
    "--alpaca-model",
    type=str,
    default="",
    help="Path to the config file of the model, for Alpaca Eval 2.0",
)
parser.add_argument(
    "--alpaca-reference-model",
    type=str,
    default="",
    help="Path to reference model, for Alpaca Eval 2.0",
)
parser.add_argument(
    "--alpaca-openai-configs",
    type=str,
    help="Path to OpenAI API config file, for Alpaca Eval 2.0",
)
parser.add_argument(
    "--from-outputs",
    action="store_true",
    help="Whether to evaluate alpaca from model or from outputs",
)


if __name__ == "__main__":
    args = parser.parse_args()

    now = str(datetime.now()).replace(" ", "_")
    if not args.no_logging:
        save_dir = f"runs/{now}"
        os.mkdir(save_dir)

    config = {
        "NUM_GENERATIONS": args.num_generations,
        "SAVE_DIR": save_dir,
        "NOW": now,
    }

    alpaca_configs = None
    if args.alpaca_eval:
        alpaca_configs = {
            "model": args.alpaca_model,
            "reference_model": args.alpaca_reference_model,
            "openai_configs": args.alpaca_openai_configs,
            "from_outputs": args.from_outputs,
        }

    alpaca_eval_values = []
    model_id = args.model_id

    # TODO: DONT HARD CODE PATHS
    csv_save_path = "."

    for i in range(args.num_generations):
        print(f"Iteration: {i}")
        evaluated, val = evaluate_gpo(model_id, alpaca_eval=args.alpaca_eval, alpaca_configs=alpaca_configs)
        if evaluated:
            alpaca_eval_values.append(val)

        df = pd.DataFrame({"alpaca_eval": alpaca_eval_values})
        df.to_csv(f"{csv_save_path}/alpaca_eval_scores_{model_id}.csv")
        print(f"Saved to {csv_save_path}/alpaca_eval_scores_{model_id}.csv")

File Path: scripts/run_cpt.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Continued pretraining script for decoder language models.
"""

import logging
import random
import sys

import datasets
import torch
import transformers
from transformers import set_seed

from alignment import (
    DataArguments,
    H4ArgumentParser,
    ModelArguments,
    SFTConfig,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
)
from trl import SFTTrainer


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, SFTConfig))
    model_args, data_args, training_args = parser.parse()

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Setup logging
    ###############
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process a small summary
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )
    logger.info(f"Model parameters {model_args}")
    logger.info(f"Data parameters {data_args}")
    logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=[data_args.text_column],
    )

    logger.info(
        f"Training on the following datasets and their proportions:"
        f" {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )

    train_dataset = raw_datasets["train"] if "train" in raw_datasets else None
    eval_dataset = raw_datasets["test"] if "test" in raw_datasets else None

    if train_dataset is None:
        raise ValueError(
            "Training set must be included (so make sure that your dataset has a split with" " 'train' in the name)."
        )

    if training_args.do_eval and eval_dataset is None:
        raise ValueError("'--do_eval' enabled so make sure that your dataset has a split with 'test' in the name.")

    ################
    # Load tokenizer
    ################
    tokenizer = get_tokenizer(model_args, data_args, auto_set_chat_template=False)

    with training_args.main_process_first(desc="Log a few random samples from the processed training set"):
        for index in random.sample(range(len(raw_datasets["train"])), 3):
            logger.info(f"Sample {index} of the processed training set:\n\n{raw_datasets['train'][index]['text']}")

    #######################
    # Load pretrained model
    #######################
    logger.info("*** Load pretrained model ***")
    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    ########################
    # Initialize the Trainer
    ########################
    trainer = SFTTrainer(
        model=model_args.model_name_or_path,
        model_init_kwargs=model_kwargs,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field=data_args.text_column,
        max_seq_length=training_args.max_seq_length,
        tokenizer=tokenizer,
        packing=True,
        peft_config=get_peft_config(model_args),
        dataset_kwargs=training_args.dataset_kwargs,
    )

    ###############
    # Training loop
    ###############
    logger.info("*** Train ***")
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint

    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(train_dataset)
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(eval_dataset)
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete ***")


if __name__ == "__main__":
    main()

File Path: scripts/run_dpo.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
import random
import sys

import torch
import transformers
from transformers import AutoModelForCausalLM, set_seed

from alignment import (
    DataArguments,
    DPOConfig,
    H4ArgumentParser,
    ModelArguments,
    apply_chat_template,
    decontaminate_humaneval,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
    is_adapter_model,
)
from peft import PeftConfig, PeftModel
from trl import DPOTrainer


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, DPOConfig))
    model_args, data_args, training_args = parser.parse()

    #######
    # Setup
    #######
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    logger.info(f"Model parameters {model_args}")
    logger.info(f"Data parameters {data_args}")
    logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=[
            "messages",
            "chosen",
            "rejected",
            "prompt",
            "completion",
            "label",
        ],
    )
    logger.info(
        f"Training on the following splits: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )
    column_names = list(raw_datasets["train"].features)

    #####################################
    # Load tokenizer and process datasets
    #####################################
    data_args.truncation_side = "left"  # Truncate from left to ensure we don't lose labels in final turn
    tokenizer = get_tokenizer(model_args, data_args)

    #####################
    # Apply chat template
    #####################
    raw_datasets = raw_datasets.map(
        apply_chat_template,
        fn_kwargs={
            "tokenizer": tokenizer,
            "task": "dpo",
            "auto_insert_empty_system_msg": data_args.auto_insert_empty_system_msg,
        },
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        desc="Formatting comparisons with prompt template",
    )

    ##########################
    # Decontaminate benchmarks
    ##########################
    num_raw_train_samples = len(raw_datasets["train"])
    raw_datasets = raw_datasets.filter(
        decontaminate_humaneval,
        fn_kwargs={"text_column": "text_chosen"},
        batched=True,
        batch_size=10_000,
        num_proc=1,
        desc="Decontaminating HumanEval samples",
    )
    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets["train"])
    logger.info(
        f"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set."
    )

    # Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected
    for split in ["train", "test"]:
        raw_datasets[split] = raw_datasets[split].rename_columns(
            {
                "text_prompt": "prompt",
                "text_chosen": "chosen",
                "text_rejected": "rejected",
            }
        )

    # Log a few random samples from the training set:
    for index in random.sample(range(len(raw_datasets["train"])), 3):
        logger.info(f"Prompt sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['prompt']}")
        logger.info(f"Chosen sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['chosen']}")
        logger.info(f"Rejected sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['rejected']}")

    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    model = model_args.model_name_or_path
    if is_adapter_model(model, model_args.model_revision) is True:
        logger.info(f"Loading SFT adapter for {model_args.model_name_or_path=}")
        peft_config = PeftConfig.from_pretrained(model_args.model_name_or_path, revision=model_args.model_revision)
        model_kwargs = dict(
            revision=model_args.base_model_revision,
            trust_remote_code=model_args.trust_remote_code,
            use_flash_attention_2=model_args.use_flash_attention_2,
            torch_dtype=torch_dtype,
            use_cache=False if training_args.gradient_checkpointing else True,
            device_map=get_kbit_device_map() if quantization_config is not None else None,
            quantization_config=quantization_config,
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            **model_kwargs,
        )
        model = PeftModel.from_pretrained(
            base_model,
            model_args.model_name_or_path,
            revision=model_args.model_revision,
        )
        model_kwargs = None

    ref_model = model
    ref_model_kwargs = model_kwargs

    if model_args.use_peft is True:
        ref_model = None
        ref_model_kwargs = None

    #########################
    # Instantiate DPO trainer
    #########################
    trainer = DPOTrainer(
        model,
        ref_model,
        model_init_kwargs=model_kwargs,
        ref_model_init_kwargs=ref_model_kwargs,
        args=training_args,
        beta=training_args.beta,
        train_dataset=raw_datasets["train"],
        eval_dataset=raw_datasets["test"],
        tokenizer=tokenizer,
        max_length=training_args.max_length,
        max_prompt_length=training_args.max_prompt_length,
        peft_config=get_peft_config(model_args),
        loss_type=training_args.loss_type,
    )

    ###############
    # Training loop
    ###############
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(raw_datasets["train"])
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    logger.info("*** Training complete ***")

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(raw_datasets["test"])
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete! ***")


if __name__ == "__main__":
    main()

File Path: scripts/run_eval_imdb.py
Content:
import argparse
import os

import numpy as np
import pandas as pd
import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

import tree


def get_positive_score(scores):
    "Extract value associated with a positive sentiment from pipeline's output"
    return dict(map(lambda x: tuple(x.values()), scores))["POSITIVE"]


def get_checkpoint_folders(checkpoint_path):
    # Get a list of all directories in the specified path
    all_folders = os.listdir(checkpoint_path)

    # Filter the directories to include only those containing "checkpoint" in their name
    checkpoint_folders = [os.path.join(checkpoint_path, folder) for folder in all_folders if "checkpoint" in folder]

    checkpoint_folders.sort(key=lambda x: int(x.split("-")[-1]))

    return checkpoint_folders


# create the top-level parser
parser = argparse.ArgumentParser()
parser.add_argument("--beta", type=float, default=0.0, help="Beta value")
parser.add_argument("--loss", type=str, help="loss name")
parser.add_argument("--checkpoint", type=str, help="lcheckpoint path")

args = parser.parse_args()
# parent_dir = os.path.dirname(args.checkpoint)

checkpoint = args.checkpoint
loss = args.loss
beta = args.beta

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    # if args.n_gpu > 0:
    torch.cuda.manual_seed_all(seed)


# Load your trained model
# checkpoints = get_checkpoint_folders(args.checkpoint)


tokenizer = AutoTokenizer.from_pretrained("lvwerra/gpt2-imdb")
# tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token


# Load reference model
ref_model_name = ""  # this can be changed to another model if needed
ref_tokenizer = AutoTokenizer.from_pretrained("lvwerra/gpt2-imdb")
# ref_tokenizer.truncation_side = "right"
ref_tokenizer.padding_side = "left"
if ref_tokenizer.pad_token is None:
    ref_tokenizer.pad_token = tokenizer.eos_token
ref_model = AutoModelForCausalLM.from_pretrained("lvwerra/gpt2-imdb")
# ref_model.load_state_dict(torch.load(ref_model_name, map_location=torch.device('cpu'))['state'])
ref_model.to("cuda")
# import ipdb; ipdb.set_trace()

sentiment_fn = pipeline(
    "sentiment-analysis",
    "siebert/sentiment-roberta-large-english",
    top_k=2,
    truncation=True,
    batch_size=256,
    device=ref_model.device,  # specify the device id here
)
# Load the imdb dataset
imdb_test = load_dataset("ZHZisZZ/imdb_preference", split="test")

# Preprocess the dataset
eval_prompts = imdb_test["prompt"]  # [" ".join(review.split()[:4]) for review in imdb_test["text"]]
inputs = tokenizer(eval_prompts, return_tensors="pt", truncation=True, padding=True)

# Prepare for batching
dataset = torch.utils.data.TensorDataset(
    inputs["input_ids"],
    inputs["attention_mask"],
)
print(len(dataset))
data_loader = DataLoader(dataset, batch_size=256)


all_checkpoints = []
all_rewards = []
all_kl_divergence = []
all_losses = []
all_betas = []
all_seeds = []

for seed in np.arange(10):
    set_seed(seed)

    # for checkpoint in checkpoints:
    total_num_items = 0
    total_reward = 0
    total_kl_divergence = 0
    kl_num_counts = 0

    all_losses.append(loss)
    all_betas.append(beta)
    all_seeds.append(seed)

    # checkpoint_nr = checkpoint.split("checkpoint-")[-1]
    # all_checkpoints.append(checkpoint_nr)
    # Load model at checkpoint
    model = AutoModelForCausalLM.from_pretrained(checkpoint)
    model.to("cuda")

    try:
        with torch.no_grad():
            for batch_input_ids, batch_attention_mask in tqdm(data_loader):
                # Generate samples from the pretrained model
                # import ipdb; ipdb.set_trace()
                batch_input_ids = batch_input_ids.cuda()
                batch_attention_mask = batch_attention_mask.cuda()
                # with torch.no_grad():
                generated_ids = model.generate(
                    batch_input_ids,
                    attention_mask=batch_attention_mask,
                    do_sample=True,
                    max_new_tokens=60,
                    pad_token_id=tokenizer.pad_token_id,
                )

                # with torch.no_grad():
                if True:
                    model_inputs = tokenizer(
                        tokenizer.batch_decode(generated_ids),
                        return_tensors="pt",
                        padding=True,
                    )
                    model_inputs = tree.map_structure(lambda x: x.to(model.device), model_inputs)
                    model_outputs = model(**model_inputs, labels=model_inputs["input_ids"])
                    model_log_probs = model_outputs.logits.log_softmax(dim=-1)

                    ref_inputs = ref_tokenizer(
                        tokenizer.batch_decode(generated_ids),
                        return_tensors="pt",
                        padding=True,
                    )
                    ref_inputs = tree.map_structure(lambda x: x.to(ref_model.device), ref_inputs)
                    ref_outputs = ref_model(**ref_inputs, labels=ref_inputs["input_ids"])
                    ref_log_probs = ref_outputs.logits.log_softmax(dim=-1)

                generated_ids = model_inputs["input_ids"]
                attention_mask = (generated_ids != tokenizer.eos_token_id).float()
                generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

                sentiments = sentiment_fn(generated_texts)
                sentiment_scores = [get_positive_score(sentiment) for sentiment in sentiments]
                total_reward += sum(sentiment_scores)
                kl_divergence = (
                    torch.nn.functional.kl_div(
                        model_log_probs,
                        ref_log_probs,
                        log_target=True,
                        reduction="none",
                    ).sum(-1)
                    * attention_mask
                )

                total_kl_divergence += kl_divergence.sum().item()
                kl_num_counts += attention_mask.sum().item()

                total_num_items += len(batch_input_ids)

        # Compute averages
        average_reward = total_reward / total_num_items
        average_kl_divergence = total_kl_divergence / kl_num_counts

        all_rewards.append(average_reward)
        all_kl_divergence.append(average_kl_divergence)
        print(f"Loss: {loss}, Beta: {beta}, Reward: {average_reward}, KL Divergence: {average_kl_divergence}")

    except Exception as e:
        print(f"Error: {e}")
        all_rewards.append(np.nan)
        all_kl_divergence.append(np.nan)
        continue

df = pd.DataFrame(
    {
        "loss": all_losses,
        "beta": all_betas,
        "reward": all_rewards,
        "kl_divergence": all_kl_divergence,
        "seed": all_seeds,
    }
)
df.to_csv(f"data/imdb_results_{loss}_{beta}.csv", index=False)

File Path: scripts/run_gpo.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
import logging
import random
import sys

import torch
import transformers
from transformers import AutoModelForCausalLM, set_seed

from alignment import (
    DataArguments,
    DPOConfig,
    GPOTrainer,
    H4ArgumentParser,
    ModelArguments,
    apply_chat_template,
    decontaminate_humaneval,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
    is_adapter_model,
)
from peft import PeftConfig, PeftModel


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, DPOConfig))
    model_args, data_args, training_args = parser.parse()
    if "2b" in training_args.output_dir:
        size = "2b"
    else:
        size = "7b"

    func = None

    if training_args.loss_type == "epo":
        with open(f"recipes/zephyr-{size}-gemma/gpo/tests.json", "r") as f:
            info = json.load(f)
        latest = info[-1]
        code = latest["code"]
        print(f"RUNNING: {latest['name']}")
        print(f"CODE:\n{code}")
        training_args.output_dir = f"data/zephyr-{size}-gemma-{latest['name']}"
        # Namespace dictionary to hold the execution context
        namespace = {}
        # Execute the function definition string within the provided namespace
        exec(code, globals(), namespace)
        # Assert that there's only one function
        names = list(namespace.keys())
        assert len(names) == 1, f"{len(names)} things in namespace"
        func = namespace[names[0]]
        assert callable(func), f"{func} is not callable"

    #######
    # Setup
    #######
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    logger.info(f"Model parameters {model_args}")
    logger.info(f"Data parameters {data_args}")
    logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=[
            "messages",
            "chosen",
            "rejected",
            "prompt",
            "completion",
            "label",
        ],
    )
    logger.info(
        f"Training on the following splits: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )
    column_names = list(raw_datasets["train"].features)

    #####################################
    # Load tokenizer and process datasets
    #####################################
    data_args.truncation_side = "left"  # Truncate from left to ensure we don't lose labels in final turn
    tokenizer = get_tokenizer(model_args, data_args)

    #####################
    # Apply chat template
    #####################
    raw_datasets = raw_datasets.map(
        apply_chat_template,
        fn_kwargs={
            "tokenizer": tokenizer,
            "task": "dpo",
            "auto_insert_empty_system_msg": data_args.auto_insert_empty_system_msg,
        },
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        desc="Formatting comparisons with prompt template",
    )

    ##########################
    # Decontaminate benchmarks
    ##########################
    num_raw_train_samples = len(raw_datasets["train"])
    raw_datasets = raw_datasets.filter(
        decontaminate_humaneval,
        fn_kwargs={"text_column": "text_chosen"},
        batched=True,
        batch_size=10_000,
        num_proc=1,
        desc="Decontaminating HumanEval samples",
    )
    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets["train"])
    logger.info(
        f"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set."
    )

    # Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected
    for split in ["train", "test"]:
        raw_datasets[split] = raw_datasets[split].rename_columns(
            {
                "text_prompt": "prompt",
                "text_chosen": "chosen",
                "text_rejected": "rejected",
            }
        )

    # Log a few random samples from the training set:
    for index in random.sample(range(len(raw_datasets["train"])), 3):
        logger.info(f"Prompt sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['prompt']}")
        logger.info(f"Chosen sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['chosen']}")
        logger.info(f"Rejected sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['rejected']}")

    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    model = model_args.model_name_or_path
    if is_adapter_model(model, model_args.model_revision) is True:
        logger.info(f"Loading SFT adapter for {model_args.model_name_or_path=}")
        peft_config = PeftConfig.from_pretrained(model_args.model_name_or_path, revision=model_args.model_revision)
        model_kwargs = dict(
            revision=model_args.base_model_revision,
            trust_remote_code=model_args.trust_remote_code,
            use_flash_attention_2=model_args.use_flash_attention_2,
            torch_dtype=torch_dtype,
            use_cache=False if training_args.gradient_checkpointing else True,
            device_map=get_kbit_device_map() if quantization_config is not None else None,
            quantization_config=quantization_config,
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            **model_kwargs,
        )
        model = PeftModel.from_pretrained(
            base_model,
            model_args.model_name_or_path,
            revision=model_args.model_revision,
        )
        model_kwargs = None

    ref_model = model
    ref_model_kwargs = model_kwargs

    if model_args.use_peft is True:
        ref_model = None
        ref_model_kwargs = None

    #########################
    # Instantiate DPO trainer
    #########################
    trainer = GPOTrainer(
        model,
        ref_model,
        model_init_kwargs=model_kwargs,
        ref_model_init_kwargs=ref_model_kwargs,
        args=training_args,
        beta=training_args.beta,
        train_dataset=raw_datasets["train"],
        eval_dataset=raw_datasets["test"],
        tokenizer=tokenizer,
        max_length=training_args.max_length,
        max_prompt_length=training_args.max_prompt_length,
        peft_config=get_peft_config(model_args),
        loss_type=training_args.loss_type,
        func=func,
    )

    ###############
    # Training loop
    ###############
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(raw_datasets["train"])
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    logger.info("*** Training complete ***")

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(raw_datasets["test"])
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete! ***")


if __name__ == "__main__":
    main()

File Path: scripts/run_gpo_imdb.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
import logging
import sys

import torch
import transformers
from transformers import AutoModelForCausalLM, set_seed

from alignment import (
    DataArguments,
    DPOConfig,
    H4ArgumentParser,
    ModelArguments,
    apply_chat_template,
    decontaminate_humaneval,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
    is_adapter_model,
)
from peft import PeftConfig, PeftModel
from src.alignment import GPOTrainer


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, DPOConfig))
    model_args, data_args, training_args = parser.parse()
    if "2b" in training_args.output_dir:
        size = "2b"
    else:
        size = "7b"
    with open(f"recipes/zephyr-{size}-gemma/gpo/tests.json", "r") as f:
        json.load(f)
    print(f"RUNNING: {training_args.loss_type}")

    #######
    # Setup
    #######
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    # logger.info(f"Model parameters {model_args}")
    # logger.info(f"Data parameters {data_args}")
    # logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=["prompt", "chosen", "responses"],
    )
    logger.info(
        f"Training on the following splits: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )
    column_names = list(raw_datasets["train"].features)
    print("test: ", column_names)

    #####################################
    # Load tokenizer and process datasets
    #####################################
    data_args.truncation_side = "left"  # Truncate from left to ensure we don't lose labels in final turn
    tokenizer = get_tokenizer(model_args, data_args)

    #####################
    # Make changes to dataset to fit OpenAI scheme
    #####################
    def imdb2openAI(example):
        prompt = example["prompt"]
        responses = example["responses"]
        chosen_idx = example["chosen"]

        chosen = [
            {"content": prompt, "role": "user"},
            {"content": responses[chosen_idx], "role": "assistant"},
        ]
        rejected = [
            {"content": prompt, "role": "user"},
            {"content": responses[1 - chosen_idx], "role": "assistant"},
        ]

        new_example = {
            "chosen": chosen,
            "rejected": rejected,
        }

        return new_example

    raw_datasets = raw_datasets.map(
        imdb2openAI,
        num_proc=data_args.preprocessing_num_workers,
        desc="Changing from IMDb to OpenAI",
    )

    #####################
    # Apply chat template
    #####################
    raw_datasets = raw_datasets.map(
        apply_chat_template,
        fn_kwargs={
            "tokenizer": tokenizer,
            "task": "dpo",
            "auto_insert_empty_system_msg": data_args.auto_insert_empty_system_msg,
        },
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=["prompt", "chosen", "rejected"],
        desc="Formatting comparisons with prompt template",
    )

    # Reduce dataset
    # raw_datasets = raw_datasets.filter(lambda x: len(x["text_prompt"]) < 450, batched=False)

    ##########################
    # Decontaminate benchmarks
    ##########################
    num_raw_train_samples = len(raw_datasets["train"])
    raw_datasets = raw_datasets.filter(
        decontaminate_humaneval,
        fn_kwargs={"text_column": "text_chosen"},
        batched=True,
        batch_size=10_000,
        num_proc=1,
        desc="Decontaminating HumanEval samples",
    )
    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets["train"])
    logger.info(
        f"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set."
    )

    # Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected
    for split in ["train", "test"]:
        raw_datasets[split] = raw_datasets[split].rename_columns(
            {
                "text_prompt": "prompt",
                "text_chosen": "chosen",
                "text_rejected": "rejected",
            }
        )

    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    model = model_args.model_name_or_path
    if is_adapter_model(model, model_args.model_revision) is True:
        logger.info(f"Loading SFT adapter for {model_args.model_name_or_path=}")
        peft_config = PeftConfig.from_pretrained(model_args.model_name_or_path, revision=model_args.model_revision)
        model_kwargs = dict(
            revision=model_args.base_model_revision,
            trust_remote_code=model_args.trust_remote_code,
            use_flash_attention_2=model_args.use_flash_attention_2,
            torch_dtype=torch_dtype,
            use_cache=False if training_args.gradient_checkpointing else True,
            device_map=get_kbit_device_map() if quantization_config is not None else None,
            quantization_config=quantization_config,
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            **model_kwargs,
        )
        model = PeftModel.from_pretrained(
            base_model,
            model_args.model_name_or_path,
            revision=model_args.model_revision,
        )
        model_kwargs = None

    ref_model = model
    ref_model_kwargs = model_kwargs

    if model_args.use_peft is True:
        ref_model = None
        ref_model_kwargs = None

    # Instantiate DPO trainer
    #########################
    trainer = GPOTrainer(
        model,
        ref_model,
        model_init_kwargs=model_kwargs,
        ref_model_init_kwargs=ref_model_kwargs,
        args=training_args,
        beta=training_args.beta,
        train_dataset=raw_datasets["train"],
        eval_dataset=raw_datasets["test"],
        tokenizer=tokenizer,
        max_length=training_args.max_length,
        max_prompt_length=training_args.max_prompt_length,
        peft_config=get_peft_config(model_args),
        loss_type=training_args.loss_type,
        func=None,
    )

    ###############
    # Training loop
    ###############
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(raw_datasets["train"])
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    logger.info("*** Training complete ***")

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(raw_datasets["test"])
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete! ***")


if __name__ == "__main__":
    main()

File Path: scripts/run_gpo_tldr.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
import logging
import sys

import torch
import transformers
from transformers import AutoModelForCausalLM, set_seed

from alignment import (
    DataArguments,
    DPOConfig,
    H4ArgumentParser,
    ModelArguments,
    apply_chat_template,
    decontaminate_humaneval,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
    is_adapter_model,
)
from peft import PeftConfig, PeftModel
from src.alignment import GPOTrainer


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, DPOConfig))
    model_args, data_args, training_args = parser.parse()
    if "2b" in training_args.output_dir:
        size = "2b"
    else:
        size = "7b"
    with open(f"recipes/zephyr-{size}-gemma/gpo/tests.json", "r") as f:
        json.load(f)
    print(f"RUNNING: {training_args.loss_type}")

    #######
    # Setup
    #######
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    # logger.info(f"Model parameters {model_args}")
    # logger.info(f"Data parameters {data_args}")
    # logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=["prompt", "chosen", "rejected"],
    )
    logger.info(
        f"Training on the following splits: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )
    column_names = list(raw_datasets["train"].features)
    print("test: ", column_names)

    #####################################
    # Load tokenizer and process datasets
    #####################################
    data_args.truncation_side = "left"  # Truncate from left to ensure we don't lose labels in final turn
    tokenizer = get_tokenizer(model_args, data_args)

    #####################
    # Make changes to dataset to fit OpenAI scheme
    #####################
    def carperAI2openAI(example):
        prompt = "example['prompt']\n\nTL;DR:"
        old_chosen = example["chosen"]
        old_rejected = example["rejected"]

        chosen = [
            {"content": prompt, "role": "user"},
            {"content": old_chosen, "role": "assistant"},
        ]
        rejected = [
            {"content": prompt, "role": "user"},
            {"content": old_rejected, "role": "assistant"},
        ]

        new_example = {
            "chosen": chosen,
            "rejected": rejected,
        }

        return new_example

    raw_datasets = raw_datasets.map(
        carperAI2openAI,
        num_proc=data_args.preprocessing_num_workers,
        desc="Changing from CarperAI to OpenAI",
    )

    #####################
    # Apply chat template
    #####################
    raw_datasets = raw_datasets.map(
        apply_chat_template,
        fn_kwargs={
            "tokenizer": tokenizer,
            "task": "dpo",
            "auto_insert_empty_system_msg": data_args.auto_insert_empty_system_msg,
        },
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        desc="Formatting comparisons with prompt template",
    )

    # Reduce dataset
    # raw_datasets = raw_datasets.filter(lambda x: len(x["text_prompt"]) < 450, batched=False)

    ##########################
    # Decontaminate benchmarks
    ##########################
    num_raw_train_samples = len(raw_datasets["train"])
    raw_datasets = raw_datasets.filter(
        decontaminate_humaneval,
        fn_kwargs={"text_column": "text_chosen"},
        batched=True,
        batch_size=10_000,
        num_proc=1,
        desc="Decontaminating HumanEval samples",
    )
    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets["train"])
    logger.info(
        f"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set."
    )

    # Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected
    for split in ["train", "test"]:
        raw_datasets[split] = raw_datasets[split].rename_columns(
            {
                "text_prompt": "prompt",
                "text_chosen": "chosen",
                "text_rejected": "rejected",
            }
        )

    # Log a few random samples from the training set:
    # for index in random.sample(range(len(raw_datasets["train"])), 3):
    #     logger.info(f"Prompt sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['prompt']}")
    #     logger.info(f"Chosen sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['chosen']}")
    #     logger.info(f"Rejected sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['rejected']}")

    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    model = model_args.model_name_or_path
    if is_adapter_model(model, model_args.model_revision) is True:
        logger.info(f"Loading SFT adapter for {model_args.model_name_or_path=}")
        peft_config = PeftConfig.from_pretrained(model_args.model_name_or_path, revision=model_args.model_revision)
        model_kwargs = dict(
            revision=model_args.base_model_revision,
            trust_remote_code=model_args.trust_remote_code,
            use_flash_attention_2=model_args.use_flash_attention_2,
            torch_dtype=torch_dtype,
            use_cache=False if training_args.gradient_checkpointing else True,
            device_map=get_kbit_device_map() if quantization_config is not None else None,
            quantization_config=quantization_config,
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            **model_kwargs,
        )
        model = PeftModel.from_pretrained(
            base_model,
            model_args.model_name_or_path,
            revision=model_args.model_revision,
        )
        model_kwargs = None

    ref_model = model
    ref_model_kwargs = model_kwargs

    if model_args.use_peft is True:
        ref_model = None
        ref_model_kwargs = None

    #########################
    # Instantiate DPO trainer
    #########################
    trainer = GPOTrainer(
        model,
        ref_model,
        model_init_kwargs=model_kwargs,
        ref_model_init_kwargs=ref_model_kwargs,
        args=training_args,
        beta=training_args.beta,
        train_dataset=raw_datasets["train"],
        eval_dataset=raw_datasets["test"],
        tokenizer=tokenizer,
        max_length=training_args.max_length,
        max_prompt_length=training_args.max_prompt_length,
        peft_config=get_peft_config(model_args),
        loss_type=training_args.loss_type,
        func=None,
    )

    ###############
    # Training loop
    ###############
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(raw_datasets["train"])
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    logger.info("*** Training complete ***")

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(raw_datasets["test"])
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete! ***")


if __name__ == "__main__":
    main()

File Path: scripts/run_mt_bench.py
Content:
import argparse
import os
import subprocess
from datetime import datetime

import pandas as pd


def evaluate_gpo(model_id, model_path, config):
    print("Running MT Bench")
    command = [
        "python",
        "gen_model_answer.py",
        "--model-path",
        model_path,
        "--model-id",
        model_id,
        "--max-new-token",
        str(config["MAX_NEW_TOKEN"]),
        "--num-gpus-total",
        str(config["NUM_GPUS"]),
        "--num-choices",
        str(config["NUM_GENERATIONS"]),
    ]

    cwd = "../FastChat/fastchat/llm_judge"
    result = subprocess.run(command, cwd=cwd)
    if result.returncode != 0:
        return (
            False,
            f"Gen Model failed with return code: {result.returncode}\n{result.stderr}",
        )

    command = [
        "python",
        "gen_judgment.py",
        "--model-list",
        model_id,
        "--parallel",
        "4",
        #'--first-n', '3',
    ]
    result = subprocess.run(command, cwd=cwd)
    if result.returncode != 0:
        return (
            False,
            f"Gen Judgemnt failed with return code: {result.returncode}\n{result.stderr}",
        )
    input_file = "../FastChat/fastchat/llm_judge/data/mt_bench/model_judgment/gpt-4_single.jsonl"

    print(f"Input file: {input_file}")
    df_all = pd.read_json(input_file, lines=True)
    df = df_all[["model", "score", "turn"]]
    df = df[df["score"] != -1]
    df = df[df["model"].isin([model_id])]

    print("\n########## First turn ##########")
    df_1 = df[df["turn"] == 1].groupby(["model", "turn"]).mean()
    print(df_1.sort_values(by="score", ascending=False))

    print("\n########## Second turn ##########")
    df_2 = df[df["turn"] == 2].groupby(["model", "turn"]).mean()
    print(df_2.sort_values(by="score", ascending=False))

    print("\n########## Average MT-Bench ##########")
    df_3 = df[["model", "score"]].groupby(["model"]).mean()
    print(df_3.sort_values(by="score", ascending=False))
    mt_bench_score = df_3.loc[model_id]["score"]

    return True, mt_bench_score


parser = argparse.ArgumentParser()
parser.add_argument(
    "--num-generations",
    type=int,
    default=5,
    help="Number of times the evaluation is to run",
)
parser.add_argument("--num-gpus", type=int, default=4, help="Number of GPUs to use")
parser.add_argument(
    "--model-id",
    type=str,
    help="Your personal model ID, does not have to match the model path name",
)
parser.add_argument(
    "--model-path",
    type=str,
    help="Model path, either to directory where the weights are saved or to a HF repo",
)
parser.add_argument("--mt-bench", action="store_true", help="Whether to evaluate on MT Bench")
parser.add_argument("--no-logging", action="store_true")
parser.add_argument("--csv-save-path", type=str, default="./", help="Path to save the CSV file")
parser.add_argument("--max-new-token", type=int, help="Max number of tokens to generate")


if __name__ == "__main__":
    args = parser.parse_args()
    assert args.num_gpus in [1, 2, 4, 8], "NUM GPUS must be 1, 2, 4 or 8"
    print(f"NUM GPUS: {args.num_gpus}")

    now = str(datetime.now()).replace(" ", "_")
    if not args.no_logging:
        save_dir = f"runs/{now}"
        os.mkdir(save_dir)

    config = {
        "NUM_GENERATIONS": args.num_generations,
        "SAVE_DIR": save_dir,
        "NOW": now,
        "NUM_GPUS": args.num_gpus,
        "MAX_NEW_TOKEN": args.max_new_token,
    }

    mt_bench_values = []
    model_id = args.model_id

    # TODO: DONT HARD CODE PATHS
    csv_save_path = "."

    for i in range(1):  # range(args.num_generations):
        print(f"Iteration: {i}")
        evaluated, val = evaluate_gpo(
            model_id,
            args.model_path,
            config,
        )
        if evaluated:
            mt_bench_values.append(val)

        df = pd.DataFrame({"mt_bench": mt_bench_values})
        df.to_csv(f"{csv_save_path}/mt_bench_scores_{model_id}-{str(args.max_new_token)}.csv")
        print(f"Saved to {csv_save_path}/mt_bench_scores_{model_id}-{str(args.max_new_token)}.csv")

File Path: scripts/run_orpo.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import random
import sys
from typing import Any, Dict

import torch
import transformers
from transformers import AutoModelForCausalLM, set_seed

from alignment import (
    DataArguments,
    H4ArgumentParser,
    ModelArguments,
    apply_chat_template,
    decontaminate_humaneval,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
)
from alignment.configs import ORPOConfig
from trl import ORPOTrainer, setup_chat_format


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, ORPOConfig))
    model_args, data_args, training_args = parser.parse()

    #######
    # Setup
    #######
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process the small summary:
    logger.info(f"Model parameters {model_args}")
    logger.info(f"Data parameters {data_args}")
    logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=[
            "prompt",
            "chosen",
            "rejected",
        ],
    )
    logger.info(
        f"Training on the following splits: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )
    column_names = list(raw_datasets["train"].features)

    #####################################
    # Load tokenizer and process datasets
    #####################################
    data_args.truncation_side = "left"  # Truncate from left to ensure we don't lose labels in final turn
    tokenizer = get_tokenizer(model_args, data_args)

    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    # For ChatML we need to add special tokens and resize the embedding layer
    if "<|im_start|>" in tokenizer.chat_template:
        model, tokenizer = setup_chat_format(model, tokenizer)

    #####################
    # Apply chat template
    #####################
    raw_datasets = raw_datasets.map(
        apply_chat_template,
        fn_kwargs={
            "tokenizer": tokenizer,
            "task": "orpo",
            "auto_insert_empty_system_msg": data_args.auto_insert_empty_system_msg,
        },
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        desc="Formatting comparisons with prompt template",
    )

    #############################
    # Filter out seq > max_length
    #############################
    if training_args.max_prompt_length is not None:
        unfiltered_train_samples = len(raw_datasets["train"])
        if "test" in raw_datasets:
            unfiltered_test_samples = len(raw_datasets["test"])

        def filter_fn(sample: Dict[str, Any]) -> Dict[str, Any]:
            prompt_length = tokenizer(
                sample["text_prompt"],
                return_tensors="pt",
            )[
                "input_ids"
            ].size(dim=-1)

            return prompt_length < training_args.max_prompt_length

        raw_datasets = raw_datasets.filter(
            filter_fn,
            desc="Filtering out the samples where len(text_prompt) > max_prompt_length",
        )

        filtered_train_samples = unfiltered_train_samples - len(raw_datasets["train"])
        logger.info(
            f"Filtered out {filtered_train_samples} training samples out of the {unfiltered_train_samples} samples."
        )
        if "test" in raw_datasets:
            filtered_test_samples = unfiltered_test_samples - len(raw_datasets["test"])
            logger.info(
                f"Filtered out {filtered_test_samples} test samples out of the {unfiltered_test_samples} samples."
            )

    ##########################
    # Decontaminate benchmarks
    ##########################
    num_raw_train_samples = len(raw_datasets["train"])
    raw_datasets = raw_datasets.filter(
        decontaminate_humaneval,
        fn_kwargs={"text_column": "text_chosen"},
        batched=True,
        batch_size=10_000,
        num_proc=1,
        desc="Decontaminating HumanEval samples",
    )
    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets["train"])
    logger.info(
        f"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set."
    )

    # Replace column names with what TRL needs, text_prompt -> prompt, text_chosen -> chosen and text_rejected -> rejected
    for split in raw_datasets.keys():
        raw_datasets[split] = raw_datasets[split].rename_columns(
            {
                "text_prompt": "prompt",
                "text_chosen": "chosen",
                "text_rejected": "rejected",
            }
        )

    # Log a few random samples from the training set:
    for index in random.sample(range(len(raw_datasets["train"])), 3):
        logger.info(f"Prompt sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['prompt']}")
        logger.info(f"Chosen sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['chosen']}")
        logger.info(f"Rejected sample {index} of the raw training set:\n\n{raw_datasets['train'][index]['rejected']}")

    ##########################
    # Instantiate ORPO trainer
    ##########################
    trainer = ORPOTrainer(
        model,
        args=training_args,
        train_dataset=raw_datasets["train"],
        eval_dataset=raw_datasets["test"] if "test" in raw_datasets else None,
        tokenizer=tokenizer,
        peft_config=get_peft_config(model_args),  # type: ignore
    )

    ###############
    # Training loop
    ###############
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(raw_datasets["train"])
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    logger.info("*** Training complete ***")

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    if trainer.is_fsdp_enabled:
        trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval and "test" in raw_datasets:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(raw_datasets["test"])
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete! ***")


if __name__ == "__main__":
    main()

File Path: scripts/run_sft.py
Content:
#!/usr/bin/env python
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Supervised fine-tuning script for decoder language models.
"""

import logging
import random
import sys

import datasets
import torch
import transformers
from transformers import AutoModelForCausalLM, set_seed

from alignment import (
    DataArguments,
    H4ArgumentParser,
    ModelArguments,
    SFTConfig,
    apply_chat_template,
    decontaminate_humaneval,
    get_checkpoint,
    get_datasets,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
)
from trl import SFTTrainer, setup_chat_format


logger = logging.getLogger(__name__)


def main():
    parser = H4ArgumentParser((ModelArguments, DataArguments, SFTConfig))
    model_args, data_args, training_args = parser.parse()

    # Set seed for reproducibility
    set_seed(training_args.seed)

    ###############
    # Setup logging
    ###############
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    # Log on each process a small summary
    logger.warning(
        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
        + f" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
    )
    logger.info(f"Model parameters {model_args}")
    logger.info(f"Data parameters {data_args}")
    logger.info(f"Training/evaluation parameters {training_args}")

    # Check for last checkpoint
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        logger.info(f"Checkpoint detected, resuming training at {last_checkpoint=}.")

    ###############
    # Load datasets
    ###############
    raw_datasets = get_datasets(
        data_args,
        splits=data_args.dataset_splits,
        configs=data_args.dataset_configs,
        columns_to_keep=[
            "messages",
            "chosen",
            "rejected",
            "prompt",
            "completion",
            "label",
        ],
    )
    logger.info(
        f"Training on the following datasets and their proportions: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}"
    )
    column_names = list(raw_datasets["train"].features)

    ################
    # Load tokenizer
    ################
    tokenizer = get_tokenizer(model_args, data_args)

    #######################
    # Load pretrained model
    #######################
    logger.info("*** Load pretrained model ***")
    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        use_flash_attention_2=model_args.use_flash_attention_2,
        torch_dtype=torch_dtype,
        use_cache=False if training_args.gradient_checkpointing else True,
        device_map=get_kbit_device_map() if quantization_config is not None else None,
        quantization_config=quantization_config,
    )

    model = model_args.model_name_or_path
    # For ChatML we need to add special tokens and resize the embedding layer
    if "<|im_start|>" in tokenizer.chat_template and "gemma-tokenizer-chatml" not in tokenizer.name_or_path:
        model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)
        model, tokenizer = setup_chat_format(model, tokenizer)
        model_kwargs = None

    #####################
    # Apply chat template
    #####################
    raw_datasets = raw_datasets.map(
        apply_chat_template,
        fn_kwargs={
            "tokenizer": tokenizer,
            "task": "sft",
            "auto_insert_empty_system_msg": data_args.auto_insert_empty_system_msg,
        },
        num_proc=data_args.preprocessing_num_workers,
        remove_columns=column_names,
        desc="Applying chat template",
    )

    ##########################
    # Decontaminate benchmarks
    ##########################
    num_raw_train_samples = len(raw_datasets["train"])
    raw_datasets = raw_datasets.filter(decontaminate_humaneval, batched=True, batch_size=10_000, num_proc=1)
    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets["train"])
    logger.info(
        f"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set."
    )

    train_dataset = raw_datasets["train"]
    eval_dataset = raw_datasets["test"]

    with training_args.main_process_first(desc="Log a few random samples from the processed training set"):
        for index in random.sample(range(len(raw_datasets["train"])), 3):
            logger.info(f"Sample {index} of the processed training set:\n\n{raw_datasets['train'][index]['text']}")

    ########################
    # Initialize the Trainer
    ########################
    trainer = SFTTrainer(
        model=model,
        model_init_kwargs=model_kwargs,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        max_seq_length=training_args.max_seq_length,
        tokenizer=tokenizer,
        packing=True,
        peft_config=get_peft_config(model_args),
        dataset_kwargs=training_args.dataset_kwargs,
    )

    ###############
    # Training loop
    ###############
    logger.info("*** Train ***")
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint
    elif last_checkpoint is not None:
        checkpoint = last_checkpoint
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
    metrics = train_result.metrics
    metrics["train_samples"] = len(train_dataset)
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    ##################################
    # Save model and create model card
    ##################################
    logger.info("*** Save model ***")
    trainer.save_model(training_args.output_dir)
    logger.info(f"Model saved to {training_args.output_dir}")

    # Save everything else on main process
    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "dataset": list(data_args.dataset_mixer.keys()),
        "dataset_tags": list(data_args.dataset_mixer.keys()),
        "tags": ["alignment-handbook"],
    }
    if trainer.accelerator.is_main_process:
        trainer.create_model_card(**kwargs)
        # Restore k,v cache for fast inference
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    ##########
    # Evaluate
    ##########
    if training_args.do_eval:
        logger.info("*** Evaluate ***")
        metrics = trainer.evaluate()
        metrics["eval_samples"] = len(eval_dataset)
        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    if training_args.push_to_hub is True:
        logger.info("Pushing to hub...")
        trainer.push_to_hub(**kwargs)

    logger.info("*** Training complete ***")


if __name__ == "__main__":
    main()

File Path: setup.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Adapted from huggingface/transformers: https://github.com/huggingface/transformers/blob/21a2d900eceeded7be9edc445b56877b95eda4ca/setup.py


import re
import shutil
from pathlib import Path

from setuptools import find_packages, setup


# Remove stale alignment.egg-info directory to avoid https://github.com/pypa/pip/issues/5466
stale_egg_info = Path(__file__).parent / "alignment.egg-info"
if stale_egg_info.exists():
    print(
        (
            "Warning: {} exists.\n\n"
            "If you recently updated alignment, this is expected,\n"
            "but it may prevent alignment from installing in editable mode.\n\n"
            "This directory is automatically generated by Python's packaging tools.\n"
            "I will remove it now.\n\n"
            "See https://github.com/pypa/pip/issues/5466 for details.\n"
        ).format(stale_egg_info)
    )
    shutil.rmtree(stale_egg_info)


# IMPORTANT: all dependencies should be listed here with their version requirements, if any.
#   * If a dependency is fast-moving (e.g. transformers), pin to the exact version
_deps = [
    "accelerate>=0.29.2",
    "bitsandbytes==0.41.2.post2",
    "black==23.1.0",
    "datasets>=2.18.0",
    "deepspeed==0.12.2",
    "einops>=0.6.1",
    "evaluate==0.4.0",
    "flake8>=6.0.0",
    "hf-doc-builder>=0.4.0",
    "hf_transfer>=0.1.4",
    "huggingface-hub>=0.19.2,<1.0",
    "isort>=5.12.0",
    "ninja>=1.11.1",
    "numpy>=1.24.2",
    "packaging>=23.0",
    "parameterized>=0.9.0",
    "peft==0.7.1",
    "protobuf<=3.20.2",  # Needed to avoid conflicts with `transformers`
    "pytest",
    "safetensors>=0.3.3",
    "sentencepiece>=0.1.99",
    "scipy",
    "tensorboard",
    "torch==2.1.2",
    "transformers>=4.39.3",
    "trl>=0.8.2",
    "jinja2>=3.0.0",
    "tqdm>=4.64.1",
]

# this is a lookup table with items like:
#
# tokenizers: "tokenizers==0.9.4"
# packaging: "packaging"
#
# some of the values are versioned whereas others aren't.
deps = {b: a for a, b in (re.findall(r"^(([^!=<>~ \[\]]+)(?:\[[^\]]+\])?(?:[!=<>~ ].*)?$)", x)[0] for x in _deps)}


def deps_list(*pkgs):
    return [deps[pkg] for pkg in pkgs]


extras = {}
extras["tests"] = deps_list("pytest", "parameterized")
extras["torch"] = deps_list("torch")
extras["quality"] = deps_list("black", "isort", "flake8")
extras["docs"] = deps_list("hf-doc-builder")
extras["dev"] = extras["docs"] + extras["quality"] + extras["tests"]

# core dependencies shared across the whole project - keep this to a bare minimum :)
install_requires = [
    deps["accelerate"],
    deps["bitsandbytes"],
    deps["einops"],
    deps["evaluate"],
    deps["datasets"],
    deps["deepspeed"],
    deps["hf_transfer"],
    deps["huggingface-hub"],
    deps["jinja2"],
    deps["ninja"],
    deps["numpy"],
    deps["packaging"],  # utilities from PyPA to e.g., compare versions
    deps["peft"],
    deps["protobuf"],
    deps["safetensors"],
    deps["sentencepiece"],
    deps["scipy"],
    deps["tensorboard"],
    deps["tqdm"],  # progress bars in model download and training scripts
    deps["transformers"],
    deps["trl"],
]

setup(
    name="alignment-handbook",
    version="0.4.0.dev0",  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)
    author="The Hugging Face team (past and future)",
    author_email="lewis@huggingface.co",
    description="The Alignment Handbook",
    long_description=open("README.md", "r", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    keywords="nlp deep learning rlhf llm",
    license="Apache",
    url="https://github.com/huggingface/alignment-handbook",
    package_dir={"": "src"},
    packages=find_packages("src"),
    zip_safe=False,
    extras_require=extras,
    python_requires=">=3.10.9",
    install_requires=install_requires,
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Intended Audience :: Education",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: Apache Software License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.10",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
)

File Path: src/alignment/__init__.py
Content:
__version__ = "0.3.0.dev0"

from .configs import (
    DataArguments,
    DPOConfig,
    H4ArgumentParser,
    ModelArguments,
    SFTConfig,
)
from .data import apply_chat_template, get_datasets
from .decontaminate import decontaminate_humaneval
from .model_utils import (
    get_checkpoint,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
    get_tokenizer,
    is_adapter_model,
)
from .gpo import GPOTrainer

File Path: src/alignment/configs.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import dataclasses
import os
import sys
from dataclasses import dataclass, field
from typing import Any, Dict, List, NewType, Optional, Tuple

import transformers
from transformers import MODEL_FOR_CAUSAL_LM_MAPPING, HfArgumentParser


MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)


DataClassType = NewType("DataClassType", Any)


class H4ArgumentParser(HfArgumentParser):
    def parse_yaml_and_args(self, yaml_arg: str, other_args: Optional[List[str]] = None) -> List[dataclass]:
        """
        Parse a YAML file and overwrite the default/loaded values with the values provided to the command line.

        Args:
            yaml_arg (`str`):
                The path to the config file used
            other_args (`List[str]`, *optional`):
                A list of strings to parse as command line arguments, e.g. ['--arg=val', '--arg2=val2'].

        Returns:
            [`List[dataclass]`]: a list of dataclasses with the values from the YAML file and the command line
        """
        arg_list = self.parse_yaml_file(os.path.abspath(yaml_arg))

        outputs = []
        # strip other args list into dict of key-value pairs
        other_args = {arg.split("=")[0].strip("-"): arg.split("=")[1] for arg in other_args}
        used_args = {}

        # overwrite the default/loaded value with the value provided to the command line
        # adapted from https://github.com/huggingface/transformers/blob/d0b5002378daabf62769159add3e7d66d3f83c3b/src/transformers/hf_argparser.py#L327
        for data_yaml, data_class in zip(arg_list, self.dataclass_types):
            keys = {f.name for f in dataclasses.fields(data_yaml) if f.init}
            inputs = {k: v for k, v in vars(data_yaml).items() if k in keys}
            for arg, val in other_args.items():
                # add only if in keys

                if arg in keys:
                    base_type = data_yaml.__dataclass_fields__[arg].type
                    inputs[arg] = val

                    # cast type for ints, floats (default to strings)
                    if base_type in [int, float]:
                        inputs[arg] = base_type(val)

                    if base_type == List[str]:
                        inputs[arg] = [str(v) for v in val.split(",")]

                    # bool of a non-empty string is True, so we manually check for bools
                    if base_type == bool:
                        if val in ["true", "True"]:
                            inputs[arg] = True
                        else:
                            inputs[arg] = False

                    # add to used-args so we can check if double add
                    if arg not in used_args:
                        used_args[arg] = val
                    else:
                        raise ValueError(f"Duplicate argument provided: {arg}, may cause unexpected behavior")

            obj = data_class(**inputs)
            outputs.append(obj)

        return outputs

    def parse(self) -> DataClassType | Tuple[DataClassType]:
        if len(sys.argv) == 2 and sys.argv[1].endswith(".yaml"):
            # If we pass only one argument to the script and it's the path to a YAML file,
            # let's parse it to get our arguments.
            output = self.parse_yaml_file(os.path.abspath(sys.argv[1]))
        # parse command line args and yaml file
        elif len(sys.argv) > 2 and sys.argv[1].endswith(".yaml"):
            output = self.parse_yaml_and_args(os.path.abspath(sys.argv[1]), sys.argv[2:])
        # parse command line args only
        else:
            output = self.parse_args_into_dataclasses()

        if len(output) == 1:
            output = output[0]
        return output


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune.
    """

    base_model_revision: Optional[str] = field(
        default=None,
        metadata={"help": ("The base model checkpoint for weights initialization with PEFT adapters.")},
    )
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization. Don't set if you want to train a model from scratch."
            )
        },
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    model_code_revision: str = field(default=None, metadata={"help": "The branch of the IFT model"})
    torch_dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the "
                "dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The path to the tokenizer. Useful if you want to use a different tokenizer to the one stored in `model_name_or_path`."
            )
        },
    )
    trust_remote_code: bool = field(default=False, metadata={"help": "Trust remote code when loading a model."})
    use_flash_attention_2: bool = field(
        default=False,
        metadata={
            "help": (
                "Whether to use flash attention 2. You must install this manually by running `pip install flash-attn --no-build-isolation`"
            )
        },
    )
    use_peft: bool = field(
        default=False,
        metadata={"help": ("Whether to use PEFT or not for training.")},
    )
    lora_r: Optional[int] = field(
        default=16,
        metadata={"help": ("LoRA R value.")},
    )
    lora_alpha: Optional[int] = field(
        default=32,
        metadata={"help": ("LoRA alpha.")},
    )
    lora_dropout: Optional[float] = field(
        default=0.05,
        metadata={"help": ("LoRA dropout.")},
    )
    lora_target_modules: Optional[List[str]] = field(
        default=None,
        metadata={"help": ("LoRA target modules.")},
    )
    lora_modules_to_save: Optional[List[str]] = field(
        default=None,
        metadata={"help": ("Model layers to unfreeze & train")},
    )
    load_in_8bit: bool = field(default=False, metadata={"help": "use 8 bit precision"})
    load_in_4bit: bool = field(default=False, metadata={"help": "use 4 bit precision"})

    bnb_4bit_quant_type: Optional[str] = field(
        default="nf4", metadata={"help": "precise the quantization type (fp4 or nf4)"}
    )
    use_bnb_nested_quant: bool = field(default=False, metadata={"help": "use nested quantization"})

    def __post_init__(self):
        if self.load_in_8bit and self.load_in_4bit:
            raise ValueError("You can't use 8 bit and 4 bit precision at the same time")


@dataclass
class DataArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    chat_template: Optional[str] = field(default=None, metadata={"help": "The chat template to use."})
    dataset_mixer: Optional[Dict[str, float]] = field(
        default=None,
        metadata={"help": ("Datasets and their proportions to be used for training ift/rl.")},
    )
    text_column: Optional[str] = field(
        default="text",
        metadata={"help": "The column name to use for the text in the dataset (only used for continued pretraining)."},
    )
    dataset_splits: Optional[List[str]] = field(
        default_factory=lambda: ["train", "test"],
        metadata={"help": ("List of train test splits to use in the dataset")},
    )
    dataset_configs: Optional[List[str]] = field(
        default=None,
        metadata={"help": "List of dataset config names. If given must be the same length as 'dataset_mixer' keys."},
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    truncation_side: Optional[str] = field(default=None, metadata={"help": "Truncation side to use for the tokenizer."})
    auto_insert_empty_system_msg: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to automatically insert an empty system message as the first message if `system` is mentioned in the chat template."
            )
        },
    )


@dataclass
class SFTConfig(transformers.TrainingArguments):
    """
    Arguments related to the training process itself. For all parameters, see: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments
    Also used for the continued pretraining task.
    """

    dataset_kwargs: Optional[Dict[str, Any]] = field(
        default=None, metadata={"help": "Dataset kwargs for the SFTTrainer"}
    )
    max_seq_length: Optional[int] = field(
        default=None,
        metadata={"help": ("Used by TRL for reward model training, which tries to read this parameter in init.")},
    )
    logging_first_step: bool = field(
        default=True,
        metadata={"help": ("Whether to log and evaluate the first global_step or not.")},
    )
    optim: Optional[str] = field(default="adamw_torch")


@dataclass
class DPOConfig(transformers.TrainingArguments):
    """
    Arguments related to the DPO training process itself. For all parameters, see: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments
    """

    beta: Optional[float] = field(
        default=0.1,
        metadata={"help": "The beta factor in DPO loss. Higher beta means less divergence from the initial policy."},
    )
    hub_model_revision: Optional[str] = field(
        default="main",
        metadata={"help": ("The Hub model branch to push the model to.")},
    )
    logging_first_step: bool = field(
        default=True,
        metadata={"help": ("Whether to log and evaluate the first global_step or not.")},
    )
    max_prompt_length: Optional[int] = field(
        default=None,
        metadata={"help": ("For DPO, the maximum length of the prompt to use for conditioning the model.")},
    )
    max_length: Optional[int] = field(
        default=None,
        metadata={"help": ("Used by TRL for reward model training, which tries to read this parameter in init.")},
    )
    optim: Optional[str] = field(default="rmsprop")
    remove_unused_columns: bool = field(default=False)
    loss_type: Optional[str] = field(default="sigmoid", metadata={"help": ("The loss type for DPO.")})


@dataclass
class ORPOConfig(transformers.TrainingArguments):
    max_length: Optional[int] = field(
        default=None,
        metadata={"help": "The maximum length of the sequences in the batch."},
    )
    max_prompt_length: Optional[int] = field(
        default=None,
        metadata={"help": "The maximum length of the prompt."},
    )
    max_completion_length: Optional[int] = field(
        default=None,
        metadata={"help": "The maximum length of the completions."},
    )

    beta: float = field(
        default=0.1,
        metadata={
            "help": "The beta factor in ORPO loss (lambda/alpha in paper/code) that is the weight of the relative loss ratio in the SFT loss."
        },
    )
    disable_dropout: bool = field(
        default=True,
        metadata={"help": "Whether or not to disable dropouts in `model`."},
    )

    label_pad_token_id: int = field(
        default=-100,
        metadata={"help": "The label pad token id."},
    )
    padding_value: Optional[int] = field(
        default=None,
        metadata={"help": "The padding value if it is different to the tokenizer's pad_token_id."},
    )
    truncation_mode: str = field(
        default="keep_end",
        metadata={"help": "The truncation mode to use, either `keep_end` or `keep_start`."},
    )

    generate_during_eval: bool = field(
        default=False,
        metadata={"help": "Whether to sample and log generations during evaluation step."},
    )
    is_encoder_decoder: Optional[bool] = field(
        default=None,
        metadata={"help": ("If no model is provided, we need to know if the model_init returns an encoder-decoder.")},
    )

    model_init_kwargs: Optional[Dict] = field(
        default=None,
        metadata={"help": ("Dict of Optional kwargs to pass when instantiating the model from a string")},
    )

    dataset_num_proc: Optional[int] = field(
        default=None,
        metadata={"help": ("The number of workers to use to tokenize the data.")},
    )

File Path: src/alignment/data.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from typing import Any, List, Literal, Optional

from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk
from datasets.builder import DatasetGenerationError

from .configs import DataArguments


DEFAULT_CHAT_TEMPLATE = "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"


def maybe_insert_system_message(messages, tokenizer):
    if messages[0]["role"] == "system":
        return

    # chat template can be one of two attributes, we check in order
    chat_template = tokenizer.chat_template
    if chat_template is None:
        chat_template = tokenizer.default_chat_template

    # confirm the jinja template refers to a system message before inserting
    if "system" in chat_template or "<|im_start|>" in chat_template:
        messages.insert(0, {"role": "system", "content": ""})


def apply_chat_template(
    example,
    tokenizer,
    task: Literal["sft", "generation", "rm", "dpo"],
    auto_insert_empty_system_msg: bool = True,
):
    if task in ["sft", "generation"]:
        messages = example["messages"]
        # We add an empty system message if there is none
        if auto_insert_empty_system_msg:
            maybe_insert_system_message(messages, tokenizer)
        example["text"] = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True if task == "generation" else False,
        )
    elif task == "rm":
        if all(k in example.keys() for k in ("chosen", "rejected")):
            chosen_messages = example["chosen"]
            rejected_messages = example["rejected"]
            # We add an empty system message if there is none
            if auto_insert_empty_system_msg:
                maybe_insert_system_message(chosen_messages, tokenizer)
                maybe_insert_system_message(rejected_messages, tokenizer)

            example["text_chosen"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)
            example["text_rejected"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)
        else:
            raise ValueError(
                f"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}"
            )
    elif task in ["dpo", "orpo"]:
        if all(k in example.keys() for k in ("chosen", "rejected")):
            if not is_openai_format(example["chosen"]) or not is_openai_format(example["rejected"]):
                raise ValueError(
                    f"Could not format example as dialogue for `{task}` task! Require OpenAI format for all messages"
                )

            # For DPO/ORPO, the inputs are triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue
            # We therefore need to extract the N-1 turns to form the prompt
            if "prompt" in example and is_openai_format(example["prompt"]):
                prompt_messages = example["prompt"]
                chosen_messages = example["chosen"]
                rejected_messages = example["rejected"]
            else:
                prompt_messages = example["chosen"][:-1]
                # Now we extract the final turn to define chosen/rejected responses
                chosen_messages = example["chosen"][-1:]
                rejected_messages = example["rejected"][-1:]

            # Prepend a system message if the first message is not a system message
            if auto_insert_empty_system_msg:
                maybe_insert_system_message(prompt_messages, tokenizer)

            example["text_prompt"] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)
            example["text_chosen"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)
            example["text_rejected"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)
        else:
            raise ValueError(
                f"Could not format example as dialogue for `{task}` task! Require either the "
                f"`[chosen, rejected]` or `[prompt, chosen, rejected]` keys but found {list(example.keys())}"
            )
    else:
        raise ValueError(
            f"Task {task} not supported, please ensure that the provided task is one of ['sft', 'generation', 'rm', 'dpo', 'orpo']"
        )
    return example


def is_openai_format(messages: Any) -> bool:
    """
    Check if the input messages are in OpenAI format.
    Args:
        messages (`Any`):
            Messages to check.
    Returns:
        `bool`: Whether the messages are in OpenAI format.
    """
    if isinstance(messages, list) and all(isinstance(message, dict) for message in messages):
        return all("role" in message and "content" in message for message in messages)
    return False


def get_datasets(
    data_config: DataArguments | dict,
    splits: Optional[List[str]] = None,
    configs: Optional[List[str]] = None,
    columns_to_keep: Optional[List[str]] = None,
    shuffle: bool = True,
) -> DatasetDict:
    """
    Loads one or more datasets with varying training set proportions.

    Args:
        data_config (`DataArguments` or `dict`):
            Dataset configuration and split proportions.
        splits (`List[str]`, *optional*, defaults to `['train', 'test']`):
            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.
        configs (Optional[List[str]], *optional*, defaults to `None`):
            List of dataset config names. If given must be the same length as 'data_config' keys.
        columns_to_keep (Optional[List[str]], *optional*, defaults to `None`):
            Column names to keep in the dataset. Useful in the datamixer to avoid schema conflicts,
            and for cpt this should be (at least) the text column.
        shuffle (`bool`, *optional*, defaults to `True`):
            Whether to shuffle the training and testing/validation data.

    Returns
        [`DatasetDict`]: The dataset dictionary containing the loaded datasets.
    """
    if type(data_config) is DataArguments:
        # Structure of the config to read the datasets and their mix
        # datasets_mixer:
        #     - 'dataset1': 0.5
        #     - 'dataset2': 0.3
        #     - 'dataset3': 0.2
        dataset_mixer = data_config.dataset_mixer
    elif isinstance(data_config, dict):
        # Structure of the input is:
        #     dataset_mixer = {
        #             "dataset1": 0.5,
        #             "dataset1": 0.3,
        #             "dataset1": 0.2,
        #         }
        dataset_mixer = data_config
    else:
        raise ValueError(f"Data config {data_config} not recognized.")

    raw_datasets = mix_datasets(
        dataset_mixer,
        splits=splits,
        configs=configs,
        columns_to_keep=columns_to_keep,
        shuffle=shuffle,
    )
    return raw_datasets


def mix_datasets(
    dataset_mixer: dict,
    splits: Optional[List[str]] = None,
    configs: Optional[List[str]] = None,
    columns_to_keep: Optional[List[str]] = None,
    shuffle=True,
) -> DatasetDict:
    """
    Loads and mixes datasets according to proportions specified in `dataset_mixer`.

    Args:
        dataset_mixer (`dict`):
            Dictionary containing the dataset names and their training proportions. By default, all test proportions are 1.
        splits (Optional[List[str]], *optional*, defaults to `None`):
            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.
        configs (Optional[List[str]], *optional*, defaults to `None`):
            List of dataset config names. If given must be the same length as 'dataset_mixer' keys.
        columns_to_keep (Optional[List[str]], *optional*, defaults to `None`):
            Column names to keep in the dataset. Useful in the datamixer to avoid schema conflicts,
            and for cpt this should be (at least) the text column.
        shuffle (`bool`, *optional*, defaults to `True`):
            Whether to shuffle the training and testing/validation data.
    """
    splits = ["train", "test"] if splits is None else splits
    configs = [None] * len(dataset_mixer) if not configs else configs
    columns_to_keep = [] if columns_to_keep is None else columns_to_keep

    if configs is not None and len(configs) != len(dataset_mixer):
        raise ValueError("The number of given dataset config names must be the same as the given number of datasets.")

    raw_datasets = DatasetDict()
    raw_train_datasets = []
    raw_val_datasets = []
    fracs = []
    for (ds, frac), ds_config in zip(dataset_mixer.items(), configs):
        fracs.append(frac)
        for split in splits:
            try:
                # Try first if dataset on a Hub repo
                dataset = load_dataset(ds, ds_config, split=split)
            except DatasetGenerationError:
                # If not, check local dataset
                dataset = load_from_disk(os.path.join(ds, split))

            # Remove redundant columns to avoid schema conflicts on load
            dataset = dataset.remove_columns([col for col in dataset.column_names if col not in columns_to_keep])
            if "train" in split:
                raw_train_datasets.append(dataset)
            elif "test" in split:
                raw_val_datasets.append(dataset)
            else:
                raise ValueError(f"Split type {split} not recognized as one of test or train.")

    if any(frac < 0 for frac in fracs):
        raise ValueError("Dataset fractions cannot be negative.")

    if len(raw_train_datasets) > 0:
        train_subsets = []
        for dataset, frac in zip(raw_train_datasets, fracs):
            train_subset = dataset.select(range(int(frac * len(dataset))))
            train_subsets.append(train_subset)
        if shuffle:
            raw_datasets["train"] = concatenate_datasets(train_subsets).shuffle(seed=42)
        else:
            raw_datasets["train"] = concatenate_datasets(train_subsets)
    # No subsampling for test datasets to enable fair comparison across models
    if len(raw_val_datasets) > 0:
        if shuffle:
            raw_datasets["test"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)
        else:
            raw_datasets["test"] = concatenate_datasets(raw_val_datasets)

    if len(raw_datasets) == 0:
        raise ValueError(
            f"Dataset {dataset_mixer} not recognized with splits {splits}. Check the dataset has been correctly formatted."
        )

    return raw_datasets

File Path: src/alignment/decontaminate.py
Content:
# coding=utf-8
# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Dict, List

from datasets import load_dataset


# HumanEval solutions that are considered simple/generic enough to be kept in the training dataset
HUMAN_EVAL_STRINGS_OK = [
    "return x + y",
    "return len(string)",
    "return n**2",
    "return " ".join(strings)",
]


def extract_docstring(prompt: str) -> str:
    if '"""' in prompt:
        if prompt.count('"""') == 2:
            return prompt.split('"""')[1].strip()
        elif prompt.count('"""') == 4:
            return prompt.split('"""')[3].strip()
        else:
            raise ValueError()
    elif "'''" in prompt:
        assert prompt.count("'''") == 2
        return prompt.split("'''")[1].strip()
    else:
        raise ValueError()


def human_eval_docstrings() -> List[str]:
    ds = load_dataset("openai_humaneval", split="test")
    docstrings = [extract_docstring(v["prompt"]) for v in ds]
    return docstrings


def load_dataset_column(dataset: str, column: str, split: str, name=None) -> List[str]:
    ds = load_dataset(dataset, split=split, name=name)
    res = [sample[column].strip() for sample in ds]
    # Only return non-empty strings
    return [sample for sample in res if len(sample) > 0]


FILTER_OUT = {
    "human_eval_docstrings": human_eval_docstrings(),
    "human_eval_solutions": [
        s
        for s in load_dataset_column("openai_humaneval", "canonical_solution", "test")
        if s not in HUMAN_EVAL_STRINGS_OK
    ],
}


def normalize_whitespace(text: str) -> str:
    return " ".join(text.split())


def decontaminate_humaneval(
    samples: List[Dict[str, Any]],
    text_column: str = "text",
    filter_out: Dict[str, List[str]] = FILTER_OUT,
) -> List[Dict[str, Any]]:
    """
    filter_out: Dict[str, List[str]] mapping from benchmark name to list of strings that need to be
    filtered-out.
    Return a list where each element is True if the corresponding file should be included in the dataset.
    Otherwise, the element is False.
    """
    output = []

    for content in samples[text_column]:
        content = normalize_whitespace(content.lower())
        matched = False
        for _, substrings in filter_out.items():
            for substring in substrings:
                if normalize_whitespace(substring.lower()) in content:
                    matched = True
                    break
            if matched:
                break
        # we keep files that are not matched
        output.append(not matched)

    return output

File Path: src/alignment/gpo.py
Content:
from typing import Tuple

import torch
import torch.nn.functional as F

from trl import DPOTrainer


class GPOTrainer(DPOTrainer):
    def __init__(self, *args, func=lambda: None, **kwargs):
        super().__init__(*args, **kwargs)
        self.func = func

    def gpo_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        if self.loss_type == "epo":
            return self.func(
                self,
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "aqfl":
            return self.adaptive_quantile_feedback_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "dbaql":
            return self.dynamic_blended_adaptive_quantile_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "aql":
            return self.adaptive_quantile_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "padll":
            return self.performance_adaptive_decay_logistic_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "cell":
            return self.combined_exp_logistic_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "lrml":
            return self.log_ratio_modulated_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "pfl":
            return self.policy_focused_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type in ["sigmoid", "dpo"]:
            return self.sigmoid_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "hinge":
            return self.hinge_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "ipo":
            return self.ipo_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "kto_pair":
            return self.kto_pair_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        elif self.loss_type == "bco_pair":
            return self.bco_pair_loss(
                policy_chosen_logps,
                policy_rejected_logps,
                reference_chosen_logps,
                reference_rejected_logps,
            )
        else:
            raise ValueError(
                f"Unknown loss type: {self.loss_type}. Should be one of ['sigmoid', 'hinge', 'ipo', 'kto_pair', 'bco_pair', 'epo']"
            )

    def dynamic_blended_adaptive_quantile_loss_old(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        import torch.nn.functional as F

        # Constants for the loss function
        starting_quantile = 0.5
        quantile_adapt_rate = 0.01
        temperature = 0.9
        dynamic_blend_rate = 1.0

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        logits_variability = logits.var()

        # Calculate an adaptive quantile based on a moving target
        starting_quantile + quantile_adapt_rate * (torch.sigmoid(logits.mean()) - starting_quantile)

        # Calculate dynamic blending coefficient based on logits variability
        dynamic_blend_coeff = torch.sigmoid(logits_variability) * dynamic_blend_rate

        # Prepare components of the blended loss
        logistic_loss = -F.logsigmoid(self.beta * logits / temperature)
        exp_loss = torch.exp(-self.beta * logits * temperature)

        # Blend the losses dynamically
        losses = dynamic_blend_coeff * logistic_loss + (1 - dynamic_blend_coeff) * exp_loss
        return losses

    def dynamic_blended_adaptive_quantile_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        import torch.nn.functional as F

        # Constants for the loss function
        temperature = 0.9
        dynamic_blend_rate = 1.0

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios

        logits = logits * self.beta

        logits_variability = (logits / 0.05).var()

        # Calculate dynamic blending coefficient based on logits variability
        dynamic_blend_coeff = torch.sigmoid(logits_variability) * dynamic_blend_rate

        # Prepare components of the blended loss
        logistic_loss = -F.logsigmoid(logits / temperature)
        exp_loss = torch.exp(-logits * temperature)

        # Blend the losses dynamically
        losses = dynamic_blend_coeff * logistic_loss + (1 - dynamic_blend_coeff) * exp_loss
        return losses

    def adaptive_quantile_loss_old(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        percentile = 0.5  # Start with the median quantile
        moving_quantile_weight = 0.01  # Weight for updating the moving quantile
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios

        moving_quantile = percentile + moving_quantile_weight * (torch.sigmoid(logits.mean()) - percentile)

        quantile_weights = torch.sigmoid(-self.beta * (logits - moving_quantile))

        logistic_losses = -F.logsigmoid(self.beta * logits)
        hinge_losses = torch.relu(1 - self.beta * logits)

        # Blend the logistic and hinge losses based on the dynamic quantile weight
        losses = quantile_weights * logistic_losses + (1 - quantile_weights) * hinge_losses
        return losses

    def adaptive_quantile_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        percentile = 0.5  # Start with the median quantile
        moving_quantile_weight = 0.01  # Weight for updating the moving quantile
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios

        logits = logits * self.beta

        moving_quantile = percentile + moving_quantile_weight * (torch.sigmoid((logits / 0.05).mean()) - percentile)

        quantile_weights = torch.sigmoid(-(logits / 0.05 - moving_quantile))

        logistic_losses = -F.logsigmoid(logits)
        hinge_losses = torch.relu(1 - logits)

        # Blend the logistic and hinge losses based on the dynamic quantile weight
        losses = quantile_weights * logistic_losses + (1 - quantile_weights) * hinge_losses
        return losses

    def adaptive_quantile_feedback_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        import torch.nn.functional as F

        quantile_update_rate = 0.05
        distance_scale = 0.1

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios

        logits = logits * self.beta

        logits_std = (logits / 0.05).std()

        adaptive_quantile = logits_std * torch.sigmoid(-logits / 0.05).mean()
        adaptive_quantile += quantile_update_rate * (torch.sigmoid((logits / 0.05).mean()) - adaptive_quantile)

        distance_from_quantile = (logits / 0.05 - adaptive_quantile).abs()
        blend_rate = torch.sigmoid(distance_scale * distance_from_quantile)

        logistic_losses = -F.logsigmoid(logits)
        hinge_losses = torch.relu(1 - logits)

        losses = blend_rate * logistic_losses + (1 - blend_rate) * hinge_losses
        return losses

    def adaptive_quantile_feedback_loss_old(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        import torch.nn.functional as F

        quantile_update_rate = 0.05
        distance_scale = 0.1

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        logits_std = logits.std()

        adaptive_quantile = logits_std * torch.sigmoid(-logits).mean()
        adaptive_quantile += quantile_update_rate * (torch.sigmoid(logits.mean()) - adaptive_quantile)

        distance_from_quantile = (logits - adaptive_quantile).abs()
        blend_rate = torch.sigmoid(distance_scale * distance_from_quantile)

        logistic_losses = -F.logsigmoid(self.beta * logits)
        hinge_losses = torch.relu(1 - self.beta * logits)

        losses = blend_rate * logistic_losses + (1 - blend_rate) * hinge_losses
        return losses

    def performance_adaptive_decay_logistic_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        base_decay = 0.9
        mismatch_penalty = 0.5  # Penalty decay for mismatched choices

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        mismatches = (logits < 0).float()  # Identify mismatches

        adaptive_decay = base_decay * (1 - mismatches * mismatch_penalty)
        weighted_losses = adaptive_decay * -F.logsigmoid(self.beta * logits)
        return weighted_losses

    def combined_exp_logistic_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        exp_losses = torch.exp(-self.beta * logits)
        log_losses = -F.logsigmoid(self.beta * logits)
        # Combine the losses with a tunable mixing coefficient
        alpha = 0.5
        losses = alpha * exp_losses + (1 - alpha) * log_losses
        return losses

    def log_ratio_modulated_loss_old(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        # Modulate the mixing coefficient based on the log ratio magnitudes
        log_ratio_modulation = torch.sigmoid(logits)
        logistic_component = -F.logsigmoid(self.beta * logits)
        exp_component = torch.exp(-self.beta * logits)
        # Blend between logistic and exponential component based on log ratio modulation
        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation
        return losses

    def log_ratio_modulated_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        tau = 0.05
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        logits = logits * self.beta
        # Modulate the mixing coefficient based on the log ratio magnitudes
        log_ratio_modulation = torch.sigmoid(logits / tau)
        logistic_component = -F.logsigmoid(logits)
        exp_component = torch.exp(-logits)
        # Blend between logistic and exponential component based on log ratio modulation
        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation
        return losses

    def policy_focused_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        focus_scale = 2.0  # Scale to emphasize or de-emphasize based on the correctness of predictions

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios

        logits = logits * self.beta

        is_correct = policy_chosen_logps > policy_rejected_logps

        logistic_losses = -F.logsigmoid(logits)
        hinge_losses = torch.relu(1 - logits)

        focused_loss = torch.where(
            is_correct,
            logistic_losses / focus_scale,  # De-emphasize correct predictions
            hinge_losses * focus_scale,  # Emphasize incorrect predictions
        )
        return focused_loss

    def policy_focused_loss_old(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        focus_scale = 2.0  # Scale to emphasize or de-emphasize based on the correctness of predictions

        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        is_correct = policy_chosen_logps > policy_rejected_logps

        logistic_losses = -F.logsigmoid(logits)
        hinge_losses = torch.relu(1 - logits)

        focused_loss = torch.where(
            is_correct,
            logistic_losses / focus_scale,  # De-emphasize correct predictions
            hinge_losses * focus_scale,  # Emphasize incorrect predictions
        )
        return focused_loss

    def sigmoid_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        losses = (
            -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
            - F.logsigmoid(-self.beta * logits) * self.label_smoothing
        )
        return losses

    def hinge_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        losses = torch.relu(1 - self.beta * logits)
        return losses

    def ipo_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps
        logits = pi_logratios - ref_logratios
        losses = (logits - 1 / (2 * self.beta)) ** 2
        return losses

    def kto_pair_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)
        rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)

        chosen_logratios = policy_chosen_logps - reference_chosen_logps
        rejected_logratios = policy_rejected_logps - reference_rejected_logps
        # As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.
        losses = torch.cat(
            (
                1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)),
                1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios)),
            ),
            0,
        )
        return losses

    def bco_pair_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> torch.FloatTensor:
        chosen_logratios = policy_chosen_logps - reference_chosen_logps
        rejected_logratios = policy_rejected_logps - reference_rejected_logps

        chosen_rewards = self.beta * chosen_logratios
        rejected_rewards = self.beta * rejected_logratios
        rewards = torch.cat((chosen_rewards, rejected_rewards), 0).mean().detach()
        self.running.update(rewards)
        delta = self.running.mean

        losses = -F.logsigmoid((self.beta * chosen_logratios) - delta) - F.logsigmoid(
            -(self.beta * rejected_logratios - delta)
        )
        return losses

    def dpo_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """Compute the DPO loss for a batch of policy and reference model log probabilities.

        Args:
            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)
            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)
            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)

        Returns:
            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).
            The losses tensor contains the DPO loss for each example in the batch.
            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.
        """
        policy_chosen_logps = policy_chosen_logps.to(self.accelerator.device)
        policy_rejected_logps = policy_rejected_logps.to(self.accelerator.device)
        reference_chosen_logps = reference_chosen_logps.to(self.accelerator.device)
        reference_rejected_logps = reference_rejected_logps.to(self.accelerator.device)
        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()
        losses = self.gpo_loss(
            policy_chosen_logps,
            policy_rejected_logps,
            reference_chosen_logps,
            reference_rejected_logps,
        )

        return losses, chosen_rewards, rejected_rewards

        # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.
        # We ignore the reference model as beta -> 0. The label_smoothing parameter encodes our uncertainty about the labels and
        # calculates a conservative DPO loss.

        # if self.loss_type == "sigmoid":
        #     losses = (
        #         -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
        #         - F.logsigmoid(-self.beta * logits) * self.label_smoothing
        #     )
        # elif self.loss_type == "hinge":
        #     losses = torch.relu(1 - self.beta * logits)
        # elif self.loss_type == "ipo":
        #     # eqn (17) of the paper where beta is the regularization parameter for the IPO loss, denoted by tau in the paper.
        #     losses = (logits - 1 / (2 * self.beta)) ** 2
        # elif self.loss_type == "kto_pair":
        #     # eqn (7) of the HALOs paper
        #     chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)
        #     rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)

        #     chosen_logratios = policy_chosen_logps - reference_chosen_logps
        #     rejected_logratios = policy_rejected_logps - reference_rejected_logps
        #     # As described in the KTO report, the KL term for chosen (rejected) is estimated using the rejected (chosen) half.
        #     losses = torch.cat(
        #         (
        #             1 - F.sigmoid(self.beta * (chosen_logratios - rejected_KL)),
        #             1 - F.sigmoid(self.beta * (chosen_KL - rejected_logratios)),
        #         ),
        #         0,
        #     )
        # elif self.loss_type == "bco_pair":
        #     chosen_logratios = policy_chosen_logps - reference_chosen_logps
        #     rejected_logratios = policy_rejected_logps - reference_rejected_logps

        #     chosen_rewards = self.beta * chosen_logratios
        #     rejected_rewards = self.beta * rejected_logratios
        #     rewards = torch.cat((chosen_rewards, rejected_rewards), 0).mean().detach()
        #     self.running.update(rewards)
        #     delta = self.running.mean

        #     losses = -F.logsigmoid((self.beta * chosen_logratios) - delta) - F.logsigmoid(
        #         -(self.beta * rejected_logratios - delta)
        #     )
        # else:
        #     raise ValueError(
        #         f"Unknown loss type: {self.loss_type}. Should be one of ['sigmoid', 'hinge', 'ipo', 'kto_pair', 'bco_pair', 'gpo']"
        #     )

File Path: src/alignment/model_utils.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
from pathlib import Path
from typing import Dict

import torch
from transformers import AutoTokenizer, BitsAndBytesConfig, PreTrainedTokenizer
from transformers.trainer_utils import get_last_checkpoint

from accelerate import Accelerator
from huggingface_hub import list_repo_files
from huggingface_hub.utils._errors import RepositoryNotFoundError
from huggingface_hub.utils._validators import HFValidationError
from peft import LoraConfig, PeftConfig

from .configs import DataArguments, DPOConfig, ModelArguments, SFTConfig
from .data import DEFAULT_CHAT_TEMPLATE


def get_current_device() -> int:
    """Get the current device. For GPU we return the local process index to enable multiple GPU training."""
    return Accelerator().local_process_index if torch.cuda.is_available() else "cpu"


def get_kbit_device_map() -> Dict[str, int] | None:
    """Useful for running inference with quantized models by setting `device_map=get_peft_device_map()`"""
    return {"": get_current_device()} if torch.cuda.is_available() else None


def get_quantization_config(model_args: ModelArguments) -> BitsAndBytesConfig | None:
    if model_args.load_in_4bit:
        compute_dtype = torch.float16
        if model_args.torch_dtype not in {"auto", None}:
            compute_dtype = getattr(torch, model_args.torch_dtype)

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,
            bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,
        )
    elif model_args.load_in_8bit:
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
        )
    else:
        quantization_config = None

    return quantization_config


def get_tokenizer(
    model_args: ModelArguments,
    data_args: DataArguments,
    auto_set_chat_template: bool = True,
) -> PreTrainedTokenizer:
    """Get the tokenizer for the model."""
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path
        if model_args.tokenizer_name_or_path is None
        else model_args.tokenizer_name_or_path,
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
    )
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    if data_args.truncation_side is not None:
        tokenizer.truncation_side = data_args.truncation_side

    # Set reasonable default for models without max length
    if tokenizer.model_max_length > 100_000:
        tokenizer.model_max_length = 2048

    if data_args.chat_template is not None:
        tokenizer.chat_template = data_args.chat_template
    elif auto_set_chat_template and tokenizer.chat_template is None and tokenizer.default_chat_template is None:
        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE

    return tokenizer


def get_peft_config(model_args: ModelArguments) -> PeftConfig | None:
    if model_args.use_peft is False:
        return None

    peft_config = LoraConfig(
        r=model_args.lora_r,
        lora_alpha=model_args.lora_alpha,
        lora_dropout=model_args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=model_args.lora_target_modules,
        modules_to_save=model_args.lora_modules_to_save,
    )

    return peft_config


def is_adapter_model(model_name_or_path: str, revision: str = "main") -> bool:
    try:
        # Try first if model on a Hub repo
        repo_files = list_repo_files(model_name_or_path, revision=revision)
    except (HFValidationError, RepositoryNotFoundError):
        # If not, check local repo
        repo_files = os.listdir(model_name_or_path)
    return "adapter_model.safetensors" in repo_files or "adapter_model.bin" in repo_files


def get_checkpoint(training_args: SFTConfig | DPOConfig) -> Path | None:
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir):
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
    return last_checkpoint

File Path: src/alignment/release.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import re

import packaging.version


REPLACE_PATTERNS = {
    "init": (
        re.compile(r'^__version__\s+=\s+"([^"]+)"\s*$', re.MULTILINE),
        '__version__ = "VERSION"\n',
    ),
    "setup": (
        re.compile(r'^(\s*)version\s*=\s*"[^"]+",', re.MULTILINE),
        r'\1version="VERSION",',
    ),
}
REPLACE_FILES = {
    "init": "src/alignment/__init__.py",
    "setup": "setup.py",
}
README_FILE = "README.md"


def update_version_in_file(fname, version, pattern):
    """Update the version in one file using a specific pattern."""
    with open(fname, "r", encoding="utf-8", newline="\n") as f:
        code = f.read()
    re_pattern, replace = REPLACE_PATTERNS[pattern]
    replace = replace.replace("VERSION", version)
    code = re_pattern.sub(replace, code)
    with open(fname, "w", encoding="utf-8", newline="\n") as f:
        f.write(code)


def global_version_update(version, patch=False):
    """Update the version in all needed files."""
    for pattern, fname in REPLACE_FILES.items():
        update_version_in_file(fname, version, pattern)


def get_version():
    """Reads the current version in the __init__."""
    with open(REPLACE_FILES["init"], "r") as f:
        code = f.read()
    default_version = REPLACE_PATTERNS["init"][0].search(code).groups()[0]
    return packaging.version.parse(default_version)


def pre_release_work(patch=False):
    """Do all the necessary pre-release steps."""
    # First let's get the default version: base version if we are in dev, bump minor otherwise.
    default_version = get_version()
    if patch and default_version.is_devrelease:
        raise ValueError("Can't create a patch version from the dev branch, checkout a released version!")
    if default_version.is_devrelease:
        default_version = default_version.base_version
    elif patch:
        default_version = f"{default_version.major}.{default_version.minor}.{default_version.micro + 1}"
    else:
        default_version = f"{default_version.major}.{default_version.minor + 1}.0"

    # Now let's ask nicely if that's the right one.
    version = input(f"Which version are you releasing? [{default_version}]")
    if len(version) == 0:
        version = default_version

    print(f"Updating version to {version}.")
    global_version_update(version, patch=patch)


def post_release_work():
    """Do all the necessary post-release steps."""
    # First let's get the current version
    current_version = get_version()
    dev_version = f"{current_version.major}.{current_version.minor + 1}.0.dev0"
    current_version = current_version.base_version

    # Check with the user we got that right.
    version = input(f"Which version are we developing now? [{dev_version}]")
    if len(version) == 0:
        version = dev_version

    print(f"Updating version to {version}.")
    global_version_update(version)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--post_release",
        action="store_true",
        help="Whether this is pre or post release.",
    )
    parser.add_argument("--patch", action="store_true", help="Whether or not this is a patch release.")
    args = parser.parse_args()
    if not args.post_release:
        pre_release_work(patch=args.patch)
    elif args.patch:
        print("Nothing to do after a patch :-)")
    else:
        post_release_work()

Output:
{
    "experimental_code": "    def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "experimental_info": "The DiscoPOP (LRML) loss function is defined as a dynamically weighted sum of logistic and exponential losses. The weighting factor is determined by a sigmoid function of the β-scaled log ratio difference (ρ), with a temperature parameter τ=0.05. The method explicitly uses `self.beta` as the beta scaling factor for the logits. The discovery pipeline uses GPT-4 (`gpt_model = \"gpt-4\"`) to iteratively propose new PyTorch loss functions. The process starts with an archive initialized with established loss functions: DPO, HINGE, IPO, and KTO, each with a placeholder fitness. Proposed loss functions undergo unit tests (`validate_code`) checking for correct function interface (one function in namespace), valid output shape (per input, e.g., (10,)), and absence of NaNs in loss and gradients during a backward pass. Valid functions are then used to finetune an LLM using `train_gpo`, which employs `accelerate launch` with a `deepspeed_zero3.yaml` config, and `gradient_accumulation_steps=16` for 4 GPUs. Model sizes are configured for 2B or 7B Gemma models. Performance is evaluated using `evaluate_gpo` which calls `FastChat/fastchat/llm_judge` to compute MT-Bench scores (average across turns 1 and 2). This performance score (`val`) is fed back to the LLM as in-context feedback, prompting it to generate the \"next one\" in the iteration."
}
